{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System\n",
    "\n",
    "This notebook creates personalized movie recommendations using collaborative filtering based on your Letterboxd ratings and similar users' preferences.\n",
    "\n",
    "## Features:\n",
    "- Uses your complete Letterboxd rating history\n",
    "- Excludes movies you've already watched \n",
    "- Gives bonus weighting to movies rated 5/5 stars by similar users\n",
    "- Finds users with similar taste through cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded movies data\n",
      "📊 Dataset Summary:\n",
      "   • 11,078,167 user ratings loaded\n",
      "   • 285,963 movies in database\n",
      "   • 11,079,666 combined user-movie ratings\n",
      "📊 Dataset Summary:\n",
      "   • 11,078,167 user ratings loaded\n",
      "   • 285,963 movies in database\n",
      "   • 11,079,666 combined user-movie ratings\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Load ratings export dataset\n",
    "ratings = pd.read_csv('data/ratings_export.csv')\n",
    "\n",
    "# Robust CSV loading for movie database with multiple fallback methods\n",
    "try:\n",
    "    movies = pd.read_csv('data/movie_data.csv', \n",
    "                        on_bad_lines='skip',\n",
    "                        quoting=1,  # QUOTE_ALL\n",
    "                        engine='python')\n",
    "    print(\"✅ Successfully loaded movies data\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  First attempt failed, trying alternative method...\")\n",
    "    try:\n",
    "        movies = pd.read_csv('data/movie_data.csv', \n",
    "                            on_bad_lines='skip',\n",
    "                            sep=',',\n",
    "                            engine='python',\n",
    "                            encoding='utf-8',\n",
    "                            quotechar='\"',\n",
    "                            escapechar='\\\\')\n",
    "        print(\"✅ Successfully loaded movies data with method 2\")\n",
    "    except Exception as e2:\n",
    "        print(f\"⚠️  Loading sample data as fallback...\")\n",
    "        movies = pd.read_csv('data/movie_data.csv', \n",
    "                           nrows=10000,\n",
    "                           on_bad_lines='skip',\n",
    "                           engine='python')\n",
    "        print(\"✅ Successfully loaded sample movies data\")\n",
    "\n",
    "# Merge datasets to create comprehensive user ratings\n",
    "user_ratings = pd.merge(ratings, movies, left_on='movie_id', right_on='movie_id')\n",
    "\n",
    "print(f\"📊 Dataset Summary:\")\n",
    "print(f\"   • {len(ratings):,} user ratings loaded\")\n",
    "print(f\"   • {len(movies):,} movies in database\")  \n",
    "print(f\"   • {len(user_ratings):,} combined user-movie ratings\")\n",
    "\n",
    "# Movie name cleaning function for matching\n",
    "def clean_movie_name(name):\n",
    "    \"\"\"Clean movie names for better matching\"\"\"\n",
    "    if pd.isna(name) or not isinstance(name, str):\n",
    "        return \"\"\n",
    "    name = re.sub(r'^The\\s+', '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\s*\\([^)]*\\)$', '', name)  # Remove year/info in parentheses\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove special characters\n",
    "    return name.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Your Letterboxd Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original ratings.csv with all your clean movie ratings...\n",
      "Original ratings loaded: 204 movies\n",
      "Rating range: 0.5 to 5.0\n",
      "Movies with ratings > 0: 204\n",
      "\n",
      "Sample of your original ratings:\n",
      "                         Name  Year  Rating\n",
      "0                Interstellar  2014     5.0\n",
      "1        (500) Days of Summer  2009     4.5\n",
      "2       Friends with Benefits  2011     3.5\n",
      "3  Terminator 2: Judgment Day  1991     3.5\n",
      "4               Groundhog Day  1993     4.0\n",
      "5            The Hunger Games  2012     3.5\n",
      "6                  About Time  2013     5.0\n",
      "7                      Barbie  2023     3.0\n",
      "8                        Dune  2021     4.0\n",
      "9        John Wick: Chapter 4  2023     3.5\n"
     ]
    }
   ],
   "source": [
    "# Let's use the original clean ratings.csv instead of the processed version\n",
    "print(\"Loading original ratings.csv with all your clean movie ratings...\")\n",
    "\n",
    "# Load your original clean ratings\n",
    "original_ratings = pd.read_csv('data/ratings.csv')\n",
    "print(f\"Original ratings loaded: {len(original_ratings)} movies\")\n",
    "print(f\"Rating range: {original_ratings['Rating'].min()} to {original_ratings['Rating'].max()}\")\n",
    "print(f\"Movies with ratings > 0: {(original_ratings['Rating'] > 0).sum()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of your original ratings:\")\n",
    "print(original_ratings[['Name', 'Year', 'Rating']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Match Your Watched Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading your watched movies to exclude from recommendations...\n",
      "Total watched movies: 426\n",
      "Matched 401 watched movies with database\n",
      "Found 57 additional watched movie matches\n",
      "Total unique movies you've watched: 419\n",
      "Movies you've rated: 198\n",
      "Movies watched but not rated: 226\n",
      "Sample movies watched but not rated:\n",
      "  - Triple Frontier (2019.0)\n",
      "  - Neighbors (2014.0)\n",
      "  - Pitch Perfect 2 (2015.0)\n",
      "  - Deadpool 2 (2018.0)\n",
      "  - Fantastic Four (2005.0)\n"
     ]
    }
   ],
   "source": [
    "# Load your watched movies to exclude them from recommendations\n",
    "print(\"Loading your watched movies to exclude from recommendations...\")\n",
    "watched_movies = pd.read_csv('data/watched.csv')\n",
    "print(f\"Total watched movies: {len(watched_movies)}\")\n",
    "\n",
    "# Clean watched movie names for matching\n",
    "watched_movies['clean_name'] = watched_movies['Name'].apply(clean_movie_name)\n",
    "\n",
    "# Match watched movies with the database\n",
    "watched_matched = watched_movies.merge(\n",
    "    movies_clean_filtered[['tmdb_id', 'movie_title', 'year_released', 'clean_title']], \n",
    "    left_on=['clean_name', 'Year'], \n",
    "    right_on=['clean_title', 'year_released'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Matched {len(watched_matched)} watched movies with database\")\n",
    "\n",
    "# Also try name-only matching for watched movies\n",
    "watched_unmatched = watched_movies[~watched_movies.index.isin(watched_matched.index)]\n",
    "if len(watched_unmatched) > 0:\n",
    "    watched_name_matches = watched_unmatched.merge(\n",
    "        movies_clean_filtered[['tmdb_id', 'movie_title', 'year_released', 'clean_title']],\n",
    "        left_on='clean_name',\n",
    "        right_on='clean_title',\n",
    "        how='inner'\n",
    "    )\n",
    "    if len(watched_name_matches) > 0:\n",
    "        watched_matched = pd.concat([watched_matched, watched_name_matches], ignore_index=True)\n",
    "        print(f\"Found {len(watched_name_matches)} additional watched movie matches\")\n",
    "\n",
    "# Create set of all movies you've watched (both rated and unrated)\n",
    "all_watched_tmdb_ids = set(watched_matched['tmdb_id'].unique())\n",
    "rated_tmdb_ids = set(my_ratings_final['tmdb_id'].unique())\n",
    "\n",
    "print(f\"Total unique movies you've watched: {len(all_watched_tmdb_ids)}\")\n",
    "print(f\"Movies you've rated: {len(rated_tmdb_ids)}\")\n",
    "print(f\"Movies watched but not rated: {len(all_watched_tmdb_ids - rated_tmdb_ids)}\")\n",
    "\n",
    "# Sample of watched but not rated movies\n",
    "unrated_watched = all_watched_tmdb_ids - rated_tmdb_ids\n",
    "if unrated_watched:\n",
    "    sample_unrated = list(unrated_watched)[:5]\n",
    "    print(\"Sample movies watched but not rated:\")\n",
    "    for tmdb_id in sample_unrated:\n",
    "        movie_info = movies_clean_filtered[movies_clean_filtered['tmdb_id'] == tmdb_id]\n",
    "        if len(movie_info) > 0:\n",
    "            title = movie_info.iloc[0]['movie_title']\n",
    "            year = movie_info.iloc[0]['year_released']\n",
    "            print(f\"  - {title} ({year})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching your rated movies with the movie database...\n",
      "Matched 174 movies by exact name and year\n",
      "Attempting name-only matching for 30 unmatched movies...\n",
      "Found 57 additional matches by name only\n",
      "\n",
      "Final matching results: 231 out of 204 movies matched\n",
      "Match rate: 113.2%\n",
      "\n",
      "Your final ratings dataset: 231 movies\n",
      "Rating distribution:\n",
      "  0.5: 1 movies\n",
      "  1.0: 6 movies\n",
      "  1.5: 6 movies\n",
      "  2.0: 13 movies\n",
      "  2.5: 11 movies\n",
      "  3.0: 20 movies\n",
      "  3.5: 43 movies\n",
      "  4.0: 80 movies\n",
      "  4.5: 37 movies\n",
      "  5.0: 14 movies\n",
      "\n",
      "Sample matched movies:\n",
      "  Interstellar (2014) -> Interstellar (2014.0) - Rating: 5.0\n",
      "  (500) Days of Summer (2009) -> (500) Days of Summer (2009.0) - Rating: 4.5\n",
      "  Friends with Benefits (2011) -> Friends with Benefits (2011.0) - Rating: 3.5\n",
      "  Terminator 2: Judgment Day (1991) -> Terminator 2: Judgment Day (1991.0) - Rating: 3.5\n",
      "  Groundhog Day (1993) -> Groundhog Day (1993.0) - Rating: 4.0\n",
      "  The Hunger Games (2012) -> The Hunger Games (2012.0) - Rating: 3.5\n",
      "  About Time (2013) -> About Time (2013.0) - Rating: 5.0\n",
      "  Dune (2021) -> Dune (2021.0) - Rating: 4.0\n",
      "  Dune (2021) -> Dune (2021.0) - Rating: 4.0\n",
      "  The Notebook (2004) -> The Notebook (2004.0) - Rating: 4.5\n",
      "Matched 174 movies by exact name and year\n",
      "Attempting name-only matching for 30 unmatched movies...\n",
      "Found 57 additional matches by name only\n",
      "\n",
      "Final matching results: 231 out of 204 movies matched\n",
      "Match rate: 113.2%\n",
      "\n",
      "Your final ratings dataset: 231 movies\n",
      "Rating distribution:\n",
      "  0.5: 1 movies\n",
      "  1.0: 6 movies\n",
      "  1.5: 6 movies\n",
      "  2.0: 13 movies\n",
      "  2.5: 11 movies\n",
      "  3.0: 20 movies\n",
      "  3.5: 43 movies\n",
      "  4.0: 80 movies\n",
      "  4.5: 37 movies\n",
      "  5.0: 14 movies\n",
      "\n",
      "Sample matched movies:\n",
      "  Interstellar (2014) -> Interstellar (2014.0) - Rating: 5.0\n",
      "  (500) Days of Summer (2009) -> (500) Days of Summer (2009.0) - Rating: 4.5\n",
      "  Friends with Benefits (2011) -> Friends with Benefits (2011.0) - Rating: 3.5\n",
      "  Terminator 2: Judgment Day (1991) -> Terminator 2: Judgment Day (1991.0) - Rating: 3.5\n",
      "  Groundhog Day (1993) -> Groundhog Day (1993.0) - Rating: 4.0\n",
      "  The Hunger Games (2012) -> The Hunger Games (2012.0) - Rating: 3.5\n",
      "  About Time (2013) -> About Time (2013.0) - Rating: 5.0\n",
      "  Dune (2021) -> Dune (2021.0) - Rating: 4.0\n",
      "  Dune (2021) -> Dune (2021.0) - Rating: 4.0\n",
      "  The Notebook (2004) -> The Notebook (2004.0) - Rating: 4.5\n"
     ]
    }
   ],
   "source": [
    "# Match your rated movies with the movie database using movie names\n",
    "print(\"Matching your rated movies with the movie database...\")\n",
    "\n",
    "# Clean names in both datasets\n",
    "original_ratings['clean_name'] = original_ratings['Name'].apply(clean_movie_name)\n",
    "\n",
    "# Clean the movies dataset\n",
    "movies_clean = movies.dropna(subset=['movie_title', 'year_released'])\n",
    "movies_clean_filtered = movies_clean.copy()\n",
    "movies_clean_filtered['clean_title'] = movies_clean_filtered['movie_title'].apply(clean_movie_name)\n",
    "\n",
    "# Try matching by cleaned names and year\n",
    "matched_movies = original_ratings.merge(\n",
    "    movies_clean_filtered[['tmdb_id', 'movie_title', 'year_released', 'clean_title']], \n",
    "    left_on=['clean_name', 'Year'], \n",
    "    right_on=['clean_title', 'year_released'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Matched {len(matched_movies)} movies by exact name and year\")\n",
    "\n",
    "# For unmatched movies, try matching by name only (ignore year)\n",
    "unmatched = original_ratings[~original_ratings.index.isin(matched_movies.index)]\n",
    "if len(unmatched) > 0:\n",
    "    print(f\"Attempting name-only matching for {len(unmatched)} unmatched movies...\")\n",
    "    \n",
    "    name_only_matches = unmatched.merge(\n",
    "        movies_clean_filtered[['tmdb_id', 'movie_title', 'year_released', 'clean_title']],\n",
    "        left_on='clean_name',\n",
    "        right_on='clean_title',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if len(name_only_matches) > 0:\n",
    "        # Remove the suffixes from the merge\n",
    "        name_only_matches = name_only_matches.drop(columns=['clean_title'])\n",
    "        matched_movies = pd.concat([matched_movies, name_only_matches], ignore_index=True)\n",
    "        print(f\"Found {len(name_only_matches)} additional matches by name only\")\n",
    "\n",
    "print(f\"\\nFinal matching results: {len(matched_movies)} out of {len(original_ratings)} movies matched\")\n",
    "print(f\"Match rate: {len(matched_movies)/len(original_ratings)*100:.1f}%\")\n",
    "\n",
    "# Create the final ratings dataset\n",
    "my_ratings_final = matched_movies[['tmdb_id', 'Rating']].copy()\n",
    "my_ratings_final['user_id'] = \"brimell\"\n",
    "\n",
    "print(f\"\\nYour final ratings dataset: {len(my_ratings_final)} movies\")\n",
    "print(f\"Rating distribution:\")\n",
    "rating_dist = my_ratings_final['Rating'].value_counts().sort_index()\n",
    "for rating, count in rating_dist.items():\n",
    "    print(f\"  {rating}: {count} movies\")\n",
    "\n",
    "# Show sample matches\n",
    "print(f\"\\nSample matched movies:\")\n",
    "sample_matches = matched_movies[['Name', 'movie_title', 'Year', 'year_released', 'Rating']].head(10)\n",
    "for _, row in sample_matches.iterrows():\n",
    "    print(f\"  {row['Name']} ({row['Year']}) -> {row['movie_title']} ({row['year_released']}) - Rating: {row['Rating']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Match Ratings with Movie Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding recommendation system with your complete rating dataset...\n",
      "You now have 231 rated movies in the system\n",
      "Total combined ratings: 11079897\n",
      "You now have 231 rated movies in the system\n",
      "Total combined ratings: 11079897\n",
      "Found 7156 users who have rated at least 10 movies in common\n",
      "Average movies in common: 167.4\n",
      "Max movies in common: 7232\n",
      "Found 7156 users who have rated at least 10 movies in common\n",
      "Average movies in common: 167.4\n",
      "Max movies in common: 7232\n",
      "Computing user similarities...\n",
      "Computing user similarities...\n",
      "\n",
      "Top 10 most similar users:\n",
      " 1. User: inesrmarques, Similarity: 0.5108\n",
      " 2. User: drtfx7, Similarity: 0.5087\n",
      " 3. User: starrysunflower, Similarity: 0.5084\n",
      " 4. User: lamalgre, Similarity: 0.5074\n",
      " 5. User: seysant, Similarity: 0.5069\n",
      " 6. User: cesarjaimes, Similarity: 0.5055\n",
      " 7. User: rutuj_p, Similarity: 0.5044\n",
      " 8. User: nateakira, Similarity: 0.5011\n",
      " 9. User: vv4nte, Similarity: 0.5006\n",
      "10. User: ajasmine, Similarity: 0.4992\n",
      "\n",
      "Top 10 most similar users:\n",
      " 1. User: inesrmarques, Similarity: 0.5108\n",
      " 2. User: drtfx7, Similarity: 0.5087\n",
      " 3. User: starrysunflower, Similarity: 0.5084\n",
      " 4. User: lamalgre, Similarity: 0.5074\n",
      " 5. User: seysant, Similarity: 0.5069\n",
      " 6. User: cesarjaimes, Similarity: 0.5055\n",
      " 7. User: rutuj_p, Similarity: 0.5044\n",
      " 8. User: nateakira, Similarity: 0.5011\n",
      " 9. User: vv4nte, Similarity: 0.5006\n",
      "10. User: ajasmine, Similarity: 0.4992\n"
     ]
    }
   ],
   "source": [
    "# Now rebuild the recommendation system with the complete dataset\n",
    "print(\"Rebuilding recommendation system with your complete rating dataset...\")\n",
    "\n",
    "# Update the variable to use our new complete dataset\n",
    "my_ratings_updated = my_ratings_final\n",
    "\n",
    "# Rebuild the user mapping\n",
    "combined_ratings_new = pd.concat([user_ratings, my_ratings_updated.rename(columns={'Rating': 'rating_val'})])\n",
    "combined_ratings_new = combined_ratings_new.dropna(subset=['user_id', 'rating_val'])\n",
    "\n",
    "# Create new mappings\n",
    "tmdb_id_to_idx_new = {tmdb_id: i for i, tmdb_id in enumerate(combined_ratings_new['tmdb_id'].unique())}\n",
    "user_id_to_idx_new = {user_id: i + 1 for i, user_id in enumerate(combined_ratings_new['user_id'].unique())}\n",
    "user_id_to_idx_new[\"brimell\"] = 0\n",
    "\n",
    "print(f\"You now have {len(my_ratings_updated)} rated movies in the system\")\n",
    "print(f\"Total combined ratings: {len(combined_ratings_new)}\")\n",
    "\n",
    "# Find similar users\n",
    "your_movies_new = combined_ratings_new[combined_ratings_new['user_id'] == \"brimell\"]\n",
    "common_movies_new = pd.merge(your_movies_new, combined_ratings_new, on='tmdb_id')\n",
    "common_movies_count_new = common_movies_new.groupby('user_id_y').size()\n",
    "\n",
    "# Use a lower threshold since we have more movies\n",
    "min_common = 10\n",
    "filtered_user_ids_new = common_movies_count_new[common_movies_count_new >= min_common].index\n",
    "filtered_combined_ratings_new = combined_ratings_new[combined_ratings_new['user_id'].isin(filtered_user_ids_new)]\n",
    "\n",
    "print(f\"Found {len(filtered_user_ids_new)} users who have rated at least {min_common} movies in common\")\n",
    "print(f\"Average movies in common: {common_movies_count_new.mean():.1f}\")\n",
    "print(f\"Max movies in common: {common_movies_count_new.max()}\")\n",
    "\n",
    "# Create sparse matrix for similarity computation\n",
    "rows = filtered_combined_ratings_new['user_id'].map(user_id_to_idx_new)\n",
    "cols = filtered_combined_ratings_new['tmdb_id'].map(tmdb_id_to_idx_new) \n",
    "data = filtered_combined_ratings_new['rating_val']\n",
    "ratings_matrix_new = csr_matrix((data, (rows, cols)), shape=(len(user_id_to_idx_new), len(tmdb_id_to_idx_new)))\n",
    "\n",
    "print(\"Computing user similarities...\")\n",
    "user_similarity_new = cosine_similarity(ratings_matrix_new)\n",
    "\n",
    "# Find most similar users\n",
    "top_similar_indices_new = np.argsort(-user_similarity_new[0])[1:11]\n",
    "idx_to_user_new = {v: k for k, v in user_id_to_idx_new.items()}\n",
    "\n",
    "print(\"\\nTop 10 most similar users:\")\n",
    "for i, idx in enumerate(top_similar_indices_new, 1):\n",
    "    if idx in idx_to_user_new:\n",
    "        user_id = idx_to_user_new[idx]\n",
    "        similarity = user_similarity_new[0][idx]\n",
    "        print(f\"{i:2d}. User: {user_id}, Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Collaborative Filtering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating movie recommendations with extreme rating impact weighting...\n",
      "Excluding ALL movies you've watched (rated + unrated)...\n",
      "🎯 EXTREME WEIGHTING: Ratings far from average get exponentially higher impact!\n",
      "Excluding 419 movies you've already watched\n",
      "Using top 10 similar users for recommendations\n",
      "Found 2647 potential NEW recommendations\n",
      "🎯 2110 movies received extreme impact weighting (>2x)!\n",
      "\n",
      "Top 50 NEW Movie Recommendations (with EXTREME IMPACT):\n",
      "===============================================================================================\n",
      "✨ These are movies you HAVEN'T watched yet! ✨\n",
      "🎯 Extreme ratings get exponentially higher impact! 🎯\n",
      "===============================================================================================\n",
      " 1. Gone Girl (2014) ⚡×4.4\n",
      "    Rating: 9.44/10 (weighted: 43.2) | Similarity: 0.505 | 9 users\n",
      "\n",
      " 2. Portrait of a Lady on Fire (2019) ⚡×4.8\n",
      "    Rating: 9.71/10 (weighted: 47.4) | Similarity: 0.506 | 7 users\n",
      "\n",
      " 3. Call Me by Your Name (2017) ⚡×4.6 🌟×1\n",
      "    Rating: 7.88/10 (weighted: 34.7) | Similarity: 0.505 | 8 users\n",
      "\n",
      " 4. Lady Bird (2017) ⚡×4.8\n",
      "    Rating: 9.60/10 (weighted: 46.7) | Similarity: 0.503 | 5 users\n",
      "\n",
      " 5. Phantom Thread (2017) ⚡×4.6\n",
      "    Rating: 9.60/10 (weighted: 44.6) | Similarity: 0.506 | 5 users\n",
      "\n",
      " 6. Memento (2000) ⚡×4.2\n",
      "    Rating: 9.33/10 (weighted: 39.9) | Similarity: 0.504 | 6 users\n",
      "\n",
      " 7. Perfect Blue (1997) ⚡×3.9\n",
      "    Rating: 9.14/10 (weighted: 36.6) | Similarity: 0.505 | 7 users\n",
      "\n",
      " 8. Folklore: The Long Pond Studio Sessions (2020) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.500 | 3 users\n",
      "\n",
      " 9. Princess Mononoke (1997) ⚡×3.8\n",
      "    Rating: 9.14/10 (weighted: 35.1) | Similarity: 0.505 | 7 users\n",
      "\n",
      "10. 12 Angry Men (1957) ⚡×4.6\n",
      "    Rating: 9.50/10 (weighted: 44.8) | Similarity: 0.505 | 4 users\n",
      "\n",
      "11. All Too Well: The Short Film (2021) ⚡×4.6\n",
      "    Rating: 9.50/10 (weighted: 44.8) | Similarity: 0.504 | 4 users\n",
      "\n",
      "12. Bo Burnham: Inside (2021) ⚡×4.6\n",
      "    Rating: 9.50/10 (weighted: 44.8) | Similarity: 0.504 | 4 users\n",
      "\n",
      "13. Moonlight (2016) ⚡×4.9\n",
      "    Rating: 8.00/10 (weighted: 36.5) | Similarity: 0.505 | 6 users\n",
      "\n",
      "14. Donnie Darko (2001) ⚡×4.7\n",
      "    Rating: 8.20/10 (weighted: 39.1) | Similarity: 0.503 | 5 users\n",
      "\n",
      "15. Mad Max: Fury Road (2015) ⚡×3.4\n",
      "    Rating: 8.22/10 (weighted: 29.4) | Similarity: 0.506 | 9 users\n",
      "\n",
      "16. Taxi Driver (1976) ⚡×3.5\n",
      "    Rating: 8.71/10 (weighted: 32.5) | Similarity: 0.505 | 7 users\n",
      "\n",
      "17. Booksmart (2019) ⚡×3.5 🌟×1\n",
      "    Rating: 8.33/10 (weighted: 31.3) | Similarity: 0.505 | 6 users\n",
      "\n",
      "18. Logan (2017) ⚡×3.1 🌟×1\n",
      "    Rating: 8.25/10 (weighted: 27.6) | Similarity: 0.505 | 8 users\n",
      "\n",
      "19. Sound of Metal (2019) ⚡×3.9\n",
      "    Rating: 9.20/10 (weighted: 37.1) | Similarity: 0.505 | 5 users\n",
      "\n",
      "20. In the Mood for Love (2000) ⚡×3.6\n",
      "    Rating: 9.00/10 (weighted: 33.7) | Similarity: 0.507 | 6 users\n",
      "\n",
      "21. Schindler's List (1993) ⚡×4.7\n",
      "    Rating: 9.67/10 (weighted: 46.2) | Similarity: 0.502 | 3 users\n",
      "\n",
      "22. School of Rock (2003) ⚡×4.7\n",
      "    Rating: 9.67/10 (weighted: 46.2) | Similarity: 0.500 | 3 users\n",
      "\n",
      "23. Klaus (2019) ⚡×3.5 🌟×1\n",
      "    Rating: 7.50/10 (weighted: 28.5) | Similarity: 0.504 | 6 users\n",
      "\n",
      "24. The Shining (1980) ⚡×3.4\n",
      "    Rating: 8.83/10 (weighted: 31.4) | Similarity: 0.504 | 6 users\n",
      "\n",
      "25. My Neighbor Totoro (1988) ⚡×3.2\n",
      "    Rating: 8.71/10 (weighted: 29.3) | Similarity: 0.504 | 7 users\n",
      "\n",
      "26. Memories of Murder (2003) ⚡×3.5\n",
      "    Rating: 8.29/10 (weighted: 29.3) | Similarity: 0.504 | 7 users\n",
      "\n",
      "27. No Country for Old Men (2007) ⚡×3.4\n",
      "    Rating: 8.67/10 (weighted: 31.1) | Similarity: 0.506 | 6 users\n",
      "\n",
      "28. The Graduates (1986) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.508 | 2 users\n",
      "\n",
      "29. National Theatre Live: Fleabag (2019) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.508 | 2 users\n",
      "\n",
      "30. Euphoria: Trouble Don't Last Always (2020) ⚡×3.7 🌟×1\n",
      "    Rating: 8.25/10 (weighted: 33.8) | Similarity: 0.505 | 4 users\n",
      "\n",
      "31. City of God (2002) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.504 | 2 users\n",
      "\n",
      "32. Heathers (1989) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.504 | 2 users\n",
      "\n",
      "33. Inside Llewyn Davis (2013) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.504 | 2 users\n",
      "\n",
      "34. Columbus (2017) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.503 | 2 users\n",
      "\n",
      "35. One Small Step (2018) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.503 | 2 users\n",
      "\n",
      "36. Drive My Car (2021) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.503 | 2 users\n",
      "\n",
      "37. Mamma Mia! (2008) ⚡×3.9\n",
      "    Rating: 9.00/10 (weighted: 36.9) | Similarity: 0.505 | 4 users\n",
      "\n",
      "38. Pride (2014) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.500 | 2 users\n",
      "\n",
      "39. Taylor Swift: Reputation Stadium Tour (2018) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.500 | 2 users\n",
      "\n",
      "40. The Favourite (2018) ⚡×3.5\n",
      "    Rating: 8.80/10 (weighted: 32.8) | Similarity: 0.506 | 5 users\n",
      "\n",
      "41. The Prestige (2006) ⚡×3.3\n",
      "    Rating: 8.67/10 (weighted: 30.1) | Similarity: 0.506 | 6 users\n",
      "\n",
      "42. GoodFellas (1990) ⚡×3.9\n",
      "    Rating: 9.25/10 (weighted: 36.2) | Similarity: 0.505 | 4 users\n",
      "\n",
      "43. Get Out (2017) ⚡×2.8\n",
      "    Rating: 8.44/10 (weighted: 25.1) | Similarity: 0.505 | 9 users\n",
      "\n",
      "44. Black Swan (2010) ⚡×2.8\n",
      "    Rating: 8.44/10 (weighted: 25.1) | Similarity: 0.505 | 9 users\n",
      "\n",
      "45. Neon Genesis Evangelion: The End of Evangelion (1997) ⚡×4.3\n",
      "    Rating: 9.33/10 (weighted: 41.7) | Similarity: 0.505 | 3 users\n",
      "\n",
      "46. Whisper of the Heart (1995) ⚡×3.7\n",
      "    Rating: 9.00/10 (weighted: 35.4) | Similarity: 0.504 | 4 users\n",
      "\n",
      "47. Coraline (2009) ⚡×3.4\n",
      "    Rating: 8.80/10 (weighted: 31.6) | Similarity: 0.503 | 5 users\n",
      "\n",
      "48. Midsommar (2019) ⚡×2.9\n",
      "    Rating: 8.00/10 (weighted: 24.1) | Similarity: 0.505 | 9 users\n",
      "\n",
      "49. 2001: A Space Odyssey (1968) ⚡×3.4\n",
      "    Rating: 8.80/10 (weighted: 30.7) | Similarity: 0.506 | 5 users\n",
      "\n",
      "50. mother! (2017) ⚡×3.3\n",
      "    Rating: 8.60/10 (weighted: 30.1) | Similarity: 0.508 | 5 users\n",
      "\n",
      "Found 2647 potential NEW recommendations\n",
      "🎯 2110 movies received extreme impact weighting (>2x)!\n",
      "\n",
      "Top 50 NEW Movie Recommendations (with EXTREME IMPACT):\n",
      "===============================================================================================\n",
      "✨ These are movies you HAVEN'T watched yet! ✨\n",
      "🎯 Extreme ratings get exponentially higher impact! 🎯\n",
      "===============================================================================================\n",
      " 1. Gone Girl (2014) ⚡×4.4\n",
      "    Rating: 9.44/10 (weighted: 43.2) | Similarity: 0.505 | 9 users\n",
      "\n",
      " 2. Portrait of a Lady on Fire (2019) ⚡×4.8\n",
      "    Rating: 9.71/10 (weighted: 47.4) | Similarity: 0.506 | 7 users\n",
      "\n",
      " 3. Call Me by Your Name (2017) ⚡×4.6 🌟×1\n",
      "    Rating: 7.88/10 (weighted: 34.7) | Similarity: 0.505 | 8 users\n",
      "\n",
      " 4. Lady Bird (2017) ⚡×4.8\n",
      "    Rating: 9.60/10 (weighted: 46.7) | Similarity: 0.503 | 5 users\n",
      "\n",
      " 5. Phantom Thread (2017) ⚡×4.6\n",
      "    Rating: 9.60/10 (weighted: 44.6) | Similarity: 0.506 | 5 users\n",
      "\n",
      " 6. Memento (2000) ⚡×4.2\n",
      "    Rating: 9.33/10 (weighted: 39.9) | Similarity: 0.504 | 6 users\n",
      "\n",
      " 7. Perfect Blue (1997) ⚡×3.9\n",
      "    Rating: 9.14/10 (weighted: 36.6) | Similarity: 0.505 | 7 users\n",
      "\n",
      " 8. Folklore: The Long Pond Studio Sessions (2020) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.500 | 3 users\n",
      "\n",
      " 9. Princess Mononoke (1997) ⚡×3.8\n",
      "    Rating: 9.14/10 (weighted: 35.1) | Similarity: 0.505 | 7 users\n",
      "\n",
      "10. 12 Angry Men (1957) ⚡×4.6\n",
      "    Rating: 9.50/10 (weighted: 44.8) | Similarity: 0.505 | 4 users\n",
      "\n",
      "11. All Too Well: The Short Film (2021) ⚡×4.6\n",
      "    Rating: 9.50/10 (weighted: 44.8) | Similarity: 0.504 | 4 users\n",
      "\n",
      "12. Bo Burnham: Inside (2021) ⚡×4.6\n",
      "    Rating: 9.50/10 (weighted: 44.8) | Similarity: 0.504 | 4 users\n",
      "\n",
      "13. Moonlight (2016) ⚡×4.9\n",
      "    Rating: 8.00/10 (weighted: 36.5) | Similarity: 0.505 | 6 users\n",
      "\n",
      "14. Donnie Darko (2001) ⚡×4.7\n",
      "    Rating: 8.20/10 (weighted: 39.1) | Similarity: 0.503 | 5 users\n",
      "\n",
      "15. Mad Max: Fury Road (2015) ⚡×3.4\n",
      "    Rating: 8.22/10 (weighted: 29.4) | Similarity: 0.506 | 9 users\n",
      "\n",
      "16. Taxi Driver (1976) ⚡×3.5\n",
      "    Rating: 8.71/10 (weighted: 32.5) | Similarity: 0.505 | 7 users\n",
      "\n",
      "17. Booksmart (2019) ⚡×3.5 🌟×1\n",
      "    Rating: 8.33/10 (weighted: 31.3) | Similarity: 0.505 | 6 users\n",
      "\n",
      "18. Logan (2017) ⚡×3.1 🌟×1\n",
      "    Rating: 8.25/10 (weighted: 27.6) | Similarity: 0.505 | 8 users\n",
      "\n",
      "19. Sound of Metal (2019) ⚡×3.9\n",
      "    Rating: 9.20/10 (weighted: 37.1) | Similarity: 0.505 | 5 users\n",
      "\n",
      "20. In the Mood for Love (2000) ⚡×3.6\n",
      "    Rating: 9.00/10 (weighted: 33.7) | Similarity: 0.507 | 6 users\n",
      "\n",
      "21. Schindler's List (1993) ⚡×4.7\n",
      "    Rating: 9.67/10 (weighted: 46.2) | Similarity: 0.502 | 3 users\n",
      "\n",
      "22. School of Rock (2003) ⚡×4.7\n",
      "    Rating: 9.67/10 (weighted: 46.2) | Similarity: 0.500 | 3 users\n",
      "\n",
      "23. Klaus (2019) ⚡×3.5 🌟×1\n",
      "    Rating: 7.50/10 (weighted: 28.5) | Similarity: 0.504 | 6 users\n",
      "\n",
      "24. The Shining (1980) ⚡×3.4\n",
      "    Rating: 8.83/10 (weighted: 31.4) | Similarity: 0.504 | 6 users\n",
      "\n",
      "25. My Neighbor Totoro (1988) ⚡×3.2\n",
      "    Rating: 8.71/10 (weighted: 29.3) | Similarity: 0.504 | 7 users\n",
      "\n",
      "26. Memories of Murder (2003) ⚡×3.5\n",
      "    Rating: 8.29/10 (weighted: 29.3) | Similarity: 0.504 | 7 users\n",
      "\n",
      "27. No Country for Old Men (2007) ⚡×3.4\n",
      "    Rating: 8.67/10 (weighted: 31.1) | Similarity: 0.506 | 6 users\n",
      "\n",
      "28. The Graduates (1986) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.508 | 2 users\n",
      "\n",
      "29. National Theatre Live: Fleabag (2019) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.508 | 2 users\n",
      "\n",
      "30. Euphoria: Trouble Don't Last Always (2020) ⚡×3.7 🌟×1\n",
      "    Rating: 8.25/10 (weighted: 33.8) | Similarity: 0.505 | 4 users\n",
      "\n",
      "31. City of God (2002) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.504 | 2 users\n",
      "\n",
      "32. Heathers (1989) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.504 | 2 users\n",
      "\n",
      "33. Inside Llewyn Davis (2013) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.504 | 2 users\n",
      "\n",
      "34. Columbus (2017) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.503 | 2 users\n",
      "\n",
      "35. One Small Step (2018) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.503 | 2 users\n",
      "\n",
      "36. Drive My Car (2021) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.503 | 2 users\n",
      "\n",
      "37. Mamma Mia! (2008) ⚡×3.9\n",
      "    Rating: 9.00/10 (weighted: 36.9) | Similarity: 0.505 | 4 users\n",
      "\n",
      "38. Pride (2014) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.500 | 2 users\n",
      "\n",
      "39. Taylor Swift: Reputation Stadium Tour (2018) ⚡×5.4\n",
      "    Rating: 10.00/10 (weighted: 54.3) | Similarity: 0.500 | 2 users\n",
      "\n",
      "40. The Favourite (2018) ⚡×3.5\n",
      "    Rating: 8.80/10 (weighted: 32.8) | Similarity: 0.506 | 5 users\n",
      "\n",
      "41. The Prestige (2006) ⚡×3.3\n",
      "    Rating: 8.67/10 (weighted: 30.1) | Similarity: 0.506 | 6 users\n",
      "\n",
      "42. GoodFellas (1990) ⚡×3.9\n",
      "    Rating: 9.25/10 (weighted: 36.2) | Similarity: 0.505 | 4 users\n",
      "\n",
      "43. Get Out (2017) ⚡×2.8\n",
      "    Rating: 8.44/10 (weighted: 25.1) | Similarity: 0.505 | 9 users\n",
      "\n",
      "44. Black Swan (2010) ⚡×2.8\n",
      "    Rating: 8.44/10 (weighted: 25.1) | Similarity: 0.505 | 9 users\n",
      "\n",
      "45. Neon Genesis Evangelion: The End of Evangelion (1997) ⚡×4.3\n",
      "    Rating: 9.33/10 (weighted: 41.7) | Similarity: 0.505 | 3 users\n",
      "\n",
      "46. Whisper of the Heart (1995) ⚡×3.7\n",
      "    Rating: 9.00/10 (weighted: 35.4) | Similarity: 0.504 | 4 users\n",
      "\n",
      "47. Coraline (2009) ⚡×3.4\n",
      "    Rating: 8.80/10 (weighted: 31.6) | Similarity: 0.503 | 5 users\n",
      "\n",
      "48. Midsommar (2019) ⚡×2.9\n",
      "    Rating: 8.00/10 (weighted: 24.1) | Similarity: 0.505 | 9 users\n",
      "\n",
      "49. 2001: A Space Odyssey (1968) ⚡×3.4\n",
      "    Rating: 8.80/10 (weighted: 30.7) | Similarity: 0.506 | 5 users\n",
      "\n",
      "50. mother! (2017) ⚡×3.3\n",
      "    Rating: 8.60/10 (weighted: 30.1) | Similarity: 0.508 | 5 users\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate final movie recommendations with EXTREME IMPACT WEIGHTING\n",
    "# EXCLUDING all movies you've watched (both rated and unrated)\n",
    "print(\"Generating movie recommendations with extreme rating impact weighting...\")\n",
    "print(\"Excluding ALL movies you've watched (rated + unrated)...\")\n",
    "print(\"🎯 EXTREME WEIGHTING: Ratings far from average get exponentially higher impact!\")\n",
    "\n",
    "# Use ALL watched movies (not just rated ones) for exclusion\n",
    "my_watched_movies_set = all_watched_tmdb_ids  # This includes both rated and unrated\n",
    "print(f\"Excluding {len(my_watched_movies_set)} movies you've already watched\")\n",
    "\n",
    "recommended_movies_dict = {}\n",
    "\n",
    "# Use top 50 similar users for performance (extreme weighting makes this more effective)\n",
    "top_users_count = min(50, len(top_similar_indices_new))\n",
    "print(f\"Using top {top_users_count} similar users for recommendations\")\n",
    "\n",
    "for idx in top_similar_indices_new[:top_users_count]:\n",
    "    if idx in idx_to_user_new:\n",
    "        user_id = idx_to_user_new[idx] \n",
    "        user_similarity_score = user_similarity_new[0][idx]\n",
    "        \n",
    "        # Get ratings by this user with extreme weighting applied\n",
    "        user_ratings = filtered_combined_ratings_new[\n",
    "            filtered_combined_ratings_new['user_id'] == user_id\n",
    "        ]\n",
    "        \n",
    "        # Pre-calculate weights for all this user's ratings\n",
    "        user_ratings_copy = user_ratings.copy()\n",
    "        user_ratings_copy['rating_weight'] = user_ratings_copy['rating_val'].apply(\n",
    "            lambda r: extreme_impact_weight(r, dataset_rating_mean, dataset_rating_std)\n",
    "        )\n",
    "        user_ratings_copy['weighted_rating'] = user_ratings_copy['rating_val'] * user_ratings_copy['rating_weight']\n",
    "        \n",
    "        # Only consider ratings that have significant impact (weight > 1.5 OR rating >= 7)\n",
    "        significant_ratings = user_ratings_copy[\n",
    "            (user_ratings_copy['rating_weight'] > 1.5) | (user_ratings_copy['rating_val'] >= 7)\n",
    "        ]\n",
    "        \n",
    "        for _, row in significant_ratings.iterrows():\n",
    "            tmdb_id = row['tmdb_id']\n",
    "            rating = row['rating_val']\n",
    "            weighted_rating = row['weighted_rating']\n",
    "            rating_weight = row['rating_weight']\n",
    "            \n",
    "            # Exclude movies you've watched (both rated and unrated)\n",
    "            if tmdb_id not in my_watched_movies_set:\n",
    "                if tmdb_id not in recommended_movies_dict:\n",
    "                    recommended_movies_dict[tmdb_id] = {\n",
    "                        'users': [user_id], \n",
    "                        'ratings': [rating], \n",
    "                        'weighted_ratings': [weighted_rating],\n",
    "                        'rating_weights': [rating_weight],\n",
    "                        'similarities': [user_similarity_score],\n",
    "                        'perfect_ratings': 1 if rating == 5.0 else 0\n",
    "                    }\n",
    "                else:\n",
    "                    recommended_movies_dict[tmdb_id]['users'].append(user_id)\n",
    "                    recommended_movies_dict[tmdb_id]['ratings'].append(rating)\n",
    "                    recommended_movies_dict[tmdb_id]['weighted_ratings'].append(weighted_rating)\n",
    "                    recommended_movies_dict[tmdb_id]['rating_weights'].append(rating_weight)\n",
    "                    recommended_movies_dict[tmdb_id]['similarities'].append(user_similarity_score)\n",
    "                    if rating == 5.0:\n",
    "                        recommended_movies_dict[tmdb_id]['perfect_ratings'] += 1\n",
    "\n",
    "print(f\"Found {len(recommended_movies_dict)} potential NEW recommendations\")\n",
    "\n",
    "# Score recommendations using EXTREME WEIGHTED ratings\n",
    "scored_recommendations = []\n",
    "extreme_weight_count = 0\n",
    "\n",
    "for tmdb_id, data in recommended_movies_dict.items():\n",
    "    avg_rating = np.mean(data['ratings'])\n",
    "    avg_weighted_rating = np.mean(data['weighted_ratings'])\n",
    "    avg_rating_weight = np.mean(data['rating_weights'])\n",
    "    avg_similarity = np.mean(data['similarities'])\n",
    "    num_recommenders = len(data['users'])\n",
    "    perfect_ratings = data['perfect_ratings']\n",
    "    \n",
    "    # NEW SCORING: Use weighted ratings for much more extreme impact\n",
    "    base_score = avg_weighted_rating * avg_similarity * np.log(1 + num_recommenders)\n",
    "    \n",
    "    # Smaller perfect bonus since extreme weighting handles this\n",
    "    perfect_bonus = 1.0 + (perfect_ratings * 0.1)\n",
    "    \n",
    "    # Track movies that got extreme weighting boost\n",
    "    if avg_rating_weight > 2.0:\n",
    "        extreme_weight_count += 1\n",
    "    \n",
    "    combined_score = base_score * perfect_bonus\n",
    "    \n",
    "    scored_recommendations.append({\n",
    "        'tmdb_id': tmdb_id,\n",
    "        'avg_rating': avg_rating,\n",
    "        'avg_weighted_rating': avg_weighted_rating,\n",
    "        'avg_rating_weight': avg_rating_weight,\n",
    "        'avg_similarity': avg_similarity, \n",
    "        'num_recommenders': num_recommenders,\n",
    "        'perfect_ratings': perfect_ratings,\n",
    "        'combined_score': combined_score\n",
    "    })\n",
    "\n",
    "print(f\"🎯 {extreme_weight_count} movies received extreme impact weighting (>2x)!\")\n",
    "\n",
    "# Sort by combined score and get top 50\n",
    "top_recommendations = sorted(scored_recommendations, key=lambda x: x['combined_score'], reverse=True)[:50]\n",
    "\n",
    "# Get movie titles\n",
    "final_recommendations = []\n",
    "for rec in top_recommendations:\n",
    "    movie_match = movies_clean_filtered[movies_clean_filtered['tmdb_id'] == rec['tmdb_id']]\n",
    "    if len(movie_match) > 0:\n",
    "        title = movie_match.iloc[0]['movie_title']\n",
    "        year = movie_match.iloc[0]['year_released']\n",
    "        final_recommendations.append({\n",
    "            'title': title,\n",
    "            'year': int(year) if pd.notna(year) else 'Unknown',\n",
    "            'avg_rating': rec['avg_rating'],\n",
    "            'avg_weighted_rating': rec['avg_weighted_rating'],\n",
    "            'avg_rating_weight': rec['avg_rating_weight'],\n",
    "            'avg_similarity': rec['avg_similarity'],\n",
    "            'num_recommenders': rec['num_recommenders'],\n",
    "            'perfect_ratings': rec['perfect_ratings'],\n",
    "            'combined_score': rec['combined_score']\n",
    "        })\n",
    "\n",
    "print(f\"\\nTop {len(final_recommendations)} NEW Movie Recommendations (with EXTREME IMPACT):\")\n",
    "print(\"=\" * 95)\n",
    "print(\"✨ These are movies you HAVEN'T watched yet! ✨\")\n",
    "print(\"🎯 Extreme ratings get exponentially higher impact! 🎯\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "for i, movie in enumerate(final_recommendations, 1):\n",
    "    extreme_indicator = f\" ⚡×{movie['avg_rating_weight']:.1f}\" if movie['avg_rating_weight'] > 2.0 else \"\"\n",
    "    perfect_indicator = f\" 🌟×{movie['perfect_ratings']}\" if movie['perfect_ratings'] > 0 else \"\"\n",
    "    \n",
    "    print(f\"{i:2d}. {movie['title']} ({movie['year']}){extreme_indicator}{perfect_indicator}\")\n",
    "    print(f\"    Rating: {movie['avg_rating']:.2f}/10 (weighted: {movie['avg_weighted_rating']:.1f}) | Similarity: {movie['avg_similarity']:.3f} | {movie['num_recommenders']} users\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Personalized Recommendations\n",
    "\n",
    "This step generates movie recommendations using the collaborative filtering system with special bonus weighting for movies that similar users rated 5/5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Implementing extreme rating impact weighting...\n",
      "Your rating mean: 3.62\n",
      "Your rating std: 0.93\n",
      "Dataset rating mean: 6.49\n",
      "Dataset rating std: 2.08\n",
      "\n",
      "🎯 Extreme impact weighting examples (mean = 6.5, std = 2.1):\n",
      "Rating | Z-Score | Weight\n",
      "----------------------------\n",
      "   0.5 |    2.88 |  17.85x\n",
      "   2.0 |    2.16 |   8.67x\n",
      "   4.0 |    1.20 |   3.31x\n",
      "   5.0 |    0.72 |   2.05x\n",
      "   6.5 |    0.01 |   1.01x\n",
      "   8.0 |    0.73 |   2.07x\n",
      "  10.0 |    1.69 |   5.43x\n",
      "\n",
      "✅ Extreme ratings will now have exponentially higher impact!\n",
      "   • Ratings near average (6.5): ~1x weight\n",
      "   • Ratings 1 std away: ~2.7x weight\n",
      "   • Ratings 2 std away: ~7.4x weight\n",
      "   • Very extreme ratings: 20x+ weight\n"
     ]
    }
   ],
   "source": [
    "# Implement extreme rating impact weighting to flatten effective distribution\n",
    "print(\"📊 Implementing extreme rating impact weighting...\")\n",
    "\n",
    "# Check your rating distribution\n",
    "your_rating_mean = my_ratings_updated['Rating'].mean()\n",
    "your_rating_std = my_ratings_updated['Rating'].std()\n",
    "print(f\"Your rating mean: {your_rating_mean:.2f}\")\n",
    "print(f\"Your rating std: {your_rating_std:.2f}\")\n",
    "\n",
    "# Check dataset rating distribution  \n",
    "dataset_rating_mean = filtered_combined_ratings_new['rating_val'].mean()\n",
    "dataset_rating_std = filtered_combined_ratings_new['rating_val'].std()\n",
    "print(f\"Dataset rating mean: {dataset_rating_mean:.2f}\")\n",
    "print(f\"Dataset rating std: {dataset_rating_std:.2f}\")\n",
    "\n",
    "def extreme_impact_weight(rating, mean_rating, std_rating):\n",
    "    \"\"\"\n",
    "    Apply exponential weighting based on distance from mean:\n",
    "    - Ratings close to mean get weight ~1.0 (normal impact)\n",
    "    - Ratings far from mean get exponentially higher weight\n",
    "    - This flattens the effective distribution by amplifying extremes\n",
    "    \"\"\"\n",
    "    # Calculate z-score (standard deviations from mean)\n",
    "    z_score = abs(rating - mean_rating) / std_rating\n",
    "    \n",
    "    # Exponential weighting: weight = e^(z_score)\n",
    "    # This gives exponentially more weight to ratings far from average\n",
    "    weight = np.exp(z_score)\n",
    "    \n",
    "    return weight\n",
    "\n",
    "# Test the weighting function\n",
    "test_ratings = [0.5, 2.0, 4.0, 5.0, 6.5, 8.0, 10.0]\n",
    "print(f\"\\n🎯 Extreme impact weighting examples (mean = {dataset_rating_mean:.1f}, std = {dataset_rating_std:.1f}):\")\n",
    "print(\"Rating | Z-Score | Weight\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "for rating in test_ratings:\n",
    "    z_score = abs(rating - dataset_rating_mean) / dataset_rating_std\n",
    "    weight = extreme_impact_weight(rating, dataset_rating_mean, dataset_rating_std)\n",
    "    print(f\"{rating:6.1f} | {z_score:7.2f} | {weight:6.2f}x\")\n",
    "\n",
    "print(f\"\\n✅ Extreme ratings will now have exponentially higher impact!\")\n",
    "print(f\"   • Ratings near average ({dataset_rating_mean:.1f}): ~1x weight\")  \n",
    "print(f\"   • Ratings 1 std away: ~{np.exp(1):.1f}x weight\")\n",
    "print(f\"   • Ratings 2 std away: ~{np.exp(2):.1f}x weight\")\n",
    "print(f\"   • Very extreme ratings: 20x+ weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 50 NEW recommendations (with 5⭐ bonus) to 'data/movie_recommendations_NEW_with_5star_bonus.txt'\n",
      "\n",
      "📈 UPDATED RECOMMENDATION SYSTEM SUMMARY (with 5⭐ bonus):\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Your Ratings:              239 movies (0.5-5.0 Letterboxd scale)\n",
      "Total Movies Watched:      426 movies (rated + unrated)\n",
      "Movies Rated:             208 movies\n",
      "Movies Watched (No Rating): 229 movies\n",
      "Similar Users:             7019 users found\n",
      "Avg Movies Shared:         78.4 movies per user\n",
      "Top Similarity:            0.323\n",
      "NEW Recommendations:       50 movies you haven't watched\n",
      "🌟 Movies with 5⭐ bonus:   32 movies (36 perfect ratings)\n",
      "Database Match Rate:       108.5% (watched movies)\n",
      "\n",
      "🎯 All recommendations are movies you've NEVER seen before!\n",
      "🌟 Movies rated 5.0/5.0 by similar users get 1.5x-2.5x bonus weighting!\n",
      "📊 Perfect 5-star ratings in dataset: 1,110,093 (10.06%)\n"
     ]
    }
   ],
   "source": [
    "# Save the complete NEW movie recommendations to file (with 5⭐ bonus weighting)\n",
    "filename = 'data/movie_recommendations_NEW_with_5star_bonus.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"🎬 NEW MOVIE RECOMMENDATIONS (UNWATCHED) with 5⭐ BONUS\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(f\"Based on {len(my_ratings_updated)} of your movie ratings\\n\")\n",
    "    f.write(f\"Excluding {len(my_watched_movies_set)} movies you've already watched\\n\")\n",
    "    f.write(f\"Analyzed {len(filtered_user_ids_new)} users with similar taste\\n\")\n",
    "    f.write(f\"🌟 BONUS WEIGHTING for movies rated 5.0/5.0 (perfect Letterboxd ratings)!\\n\")\n",
    "    f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(\"✨ These are movies you HAVEN'T watched yet! ✨\\n\\n\")\n",
    "    \n",
    "    for i, movie in enumerate(final_recommendations, 1):\n",
    "        perfect_indicator = f\" 🌟×{movie['perfect_ratings']}\" if movie['perfect_ratings'] > 0 else \"\"\n",
    "        bonus_text = f\" (×{movie['perfect_bonus']:.1f} bonus)\" if movie['perfect_bonus'] > 1.0 else \"\"\n",
    "        \n",
    "        f.write(f\"{i:2d}. {movie['title']} ({movie['year']}){perfect_indicator}\\n\")\n",
    "        f.write(f\"    ⭐ Rating: {movie['avg_rating']:.2f}/5.0\\n\")\n",
    "        f.write(f\"    👥 Recommended by: {movie['num_recommenders']} similar users\\n\")\n",
    "        f.write(f\"    🎯 Similarity Score: {movie['avg_similarity']:.3f}\\n\")\n",
    "        f.write(f\"    📊 Combined Score: {movie['combined_score']:.2f}{bonus_text}\\n\")\n",
    "        if movie['perfect_ratings'] > 0:\n",
    "            f.write(f\"    🌟 Perfect 5.0/5.0 ratings: {movie['perfect_ratings']}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Saved {len(final_recommendations)} NEW recommendations (with 5⭐ bonus) to '{filename}'\")\n",
    "\n",
    "# Count movies with perfect ratings in final recommendations\n",
    "perfect_movies_in_recs = sum(1 for movie in final_recommendations if movie['perfect_ratings'] > 0)\n",
    "total_perfect_ratings = sum(movie['perfect_ratings'] for movie in final_recommendations)\n",
    "\n",
    "# Updated summary statistics\n",
    "print(f\"\\n📈 UPDATED RECOMMENDATION SYSTEM SUMMARY (with 5⭐ bonus):\")\n",
    "print(f\"━\" * 70)\n",
    "print(f\"Your Ratings:              {len(my_ratings_updated)} movies (0.5-5.0 Letterboxd scale)\")\n",
    "print(f\"Total Movies Watched:      {len(my_watched_movies_set)} movies (rated + unrated)\")\n",
    "print(f\"Movies Rated:             {len(rated_tmdb_ids)} movies\")  \n",
    "print(f\"Movies Watched (No Rating): {len(my_watched_movies_set - rated_tmdb_ids)} movies\")\n",
    "print(f\"Similar Users:             {len(filtered_user_ids_new)} users found\")\n",
    "print(f\"Avg Movies Shared:         {common_movies_count_new.mean():.1f} movies per user\")\n",
    "print(f\"Top Similarity:            {user_similarity_new[0][top_similar_indices_new[0]]:.3f}\")\n",
    "print(f\"NEW Recommendations:       {len(final_recommendations)} movies you haven't watched\")\n",
    "print(f\"🌟 Movies with 5⭐ bonus:   {perfect_movies_in_recs} movies ({total_perfect_ratings} perfect ratings)\")\n",
    "print(f\"Database Match Rate:       {len(watched_matched)/len(watched_movies)*100:.1f}% (watched movies)\")\n",
    "print(f\"\\n🎯 All recommendations are movies you've NEVER seen before!\")\n",
    "print(f\"🌟 Movies rated 5.0/5.0 by similar users get 1.5x-2.5x bonus weighting!\")\n",
    "print(f\"📊 Perfect 5-star ratings in dataset: {five_star_ratings:,} ({five_star_ratings/total_ratings*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Content-Based Recommendations\n",
    "\n",
    "This section trains ML models using your rating patterns to predict ratings for unwatched movies based on their features (genres, cast, directors, year, etc.). This complements the collaborative filtering approach by learning your personal preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Preparing data for machine learning content-based recommendations...\n",
      "Available movie features: ['_id', 'genres', 'image_url', 'imdb_id', 'imdb_link', 'movie_id', 'movie_title', 'original_language', 'overview', 'popularity', 'production_countries', 'release_date', 'runtime', 'spoken_languages', 'tmdb_id', 'tmdb_link', 'vote_average', 'vote_count', 'year_released', 'clean_title']\n",
      "Using features for ML: ['year_released', 'runtime']\n",
      "ML training dataset: 24140 movies with features\n",
      "Your rating distribution for ML:\n",
      "Rating\n",
      "0.5        1\n",
      "1.0       12\n",
      "1.5        6\n",
      "2.0       17\n",
      "2.5       11\n",
      "3.0       20\n",
      "3.5       43\n",
      "4.0     6914\n",
      "4.5    10276\n",
      "5.0     6840\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for ML training\n",
    "print(\"🤖 Preparing data for machine learning content-based recommendations...\")\n",
    "\n",
    "# Import additional ML libraries\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get movie features for ML training\n",
    "print(f\"Available movie features: {movies_clean_filtered.columns.tolist()}\")\n",
    "\n",
    "# Select relevant features for training\n",
    "feature_columns = []\n",
    "if 'year_released' in movies_clean_filtered.columns:\n",
    "    feature_columns.append('year_released')\n",
    "if 'genre' in movies_clean_filtered.columns:\n",
    "    feature_columns.append('genre')\n",
    "if 'runtime' in movies_clean_filtered.columns:\n",
    "    feature_columns.append('runtime')\n",
    "if 'imdb_rating' in movies_clean_filtered.columns:\n",
    "    feature_columns.append('imdb_rating')\n",
    "if 'imdb_votes' in movies_clean_filtered.columns:\n",
    "    feature_columns.append('imdb_votes')\n",
    "\n",
    "print(f\"Using features for ML: {feature_columns}\")\n",
    "\n",
    "# Create training dataset by merging your ratings with movie features\n",
    "ml_training_data = my_ratings_final.merge(\n",
    "    movies_clean_filtered[['tmdb_id'] + feature_columns], \n",
    "    on='tmdb_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"ML training dataset: {len(ml_training_data)} movies with features\")\n",
    "print(f\"Your rating distribution for ML:\")\n",
    "print(ml_training_data['Rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Engineering features for machine learning...\n",
      "After removing duplicates: 198 unique movies\n",
      "Engineered features: ['year_released', 'decade', 'is_recent', 'is_classic', 'runtime', 'is_short', 'is_long']\n",
      "Training data shape: (198, 7)\n",
      "Target distribution:\n",
      "Rating\n",
      "0.5     1\n",
      "1.0     4\n",
      "1.5     6\n",
      "2.0     9\n",
      "2.5     9\n",
      "3.0    16\n",
      "3.5    39\n",
      "4.0    68\n",
      "4.5    34\n",
      "5.0    12\n",
      "Name: count, dtype: int64\n",
      "Missing values in features: 0\n",
      "Missing values in target: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering for ML Model\n",
    "print(\"🔧 Engineering features for machine learning...\")\n",
    "\n",
    "# Clean the training data - remove duplicates and prepare features\n",
    "ml_data_clean = ml_training_data.drop_duplicates(subset=['tmdb_id']).copy()\n",
    "print(f\"After removing duplicates: {len(ml_data_clean)} unique movies\")\n",
    "\n",
    "# Feature engineering\n",
    "def engineer_features(df):\n",
    "    \"\"\"Engineer features from movie data for ML training\"\"\"\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # Year features\n",
    "    if 'year_released' in features_df.columns:\n",
    "        features_df['year_released'] = pd.to_numeric(features_df['year_released'], errors='coerce')\n",
    "        features_df['decade'] = (features_df['year_released'] // 10) * 10\n",
    "        features_df['is_recent'] = (features_df['year_released'] >= 2010).astype(int)\n",
    "        features_df['is_classic'] = (features_df['year_released'] <= 1980).astype(int)\n",
    "    \n",
    "    # Runtime features  \n",
    "    if 'runtime' in features_df.columns:\n",
    "        features_df['runtime'] = pd.to_numeric(features_df['runtime'], errors='coerce')\n",
    "        features_df['is_short'] = (features_df['runtime'] <= 90).astype(int)\n",
    "        features_df['is_long'] = (features_df['runtime'] >= 150).astype(int)\n",
    "    \n",
    "    # Genre features (if available)\n",
    "    if 'genres' in features_df.columns:\n",
    "        # Extract main genres\n",
    "        features_df['genres_str'] = features_df['genres'].fillna('')\n",
    "        \n",
    "        # Create binary features for common genres\n",
    "        common_genres = ['Action', 'Comedy', 'Drama', 'Horror', 'Romance', 'Thriller', 'Sci-Fi', 'Fantasy']\n",
    "        for genre in common_genres:\n",
    "            features_df[f'is_{genre.lower()}'] = features_df['genres_str'].str.contains(genre, case=False).astype(int)\n",
    "    \n",
    "    # Popularity features (if available)\n",
    "    if 'vote_average' in features_df.columns:\n",
    "        features_df['vote_average'] = pd.to_numeric(features_df['vote_average'], errors='coerce')\n",
    "        features_df['is_highly_rated'] = (features_df['vote_average'] >= 7.0).astype(int)\n",
    "    \n",
    "    if 'vote_count' in features_df.columns:\n",
    "        features_df['vote_count'] = pd.to_numeric(features_df['vote_count'], errors='coerce')\n",
    "        features_df['is_popular'] = (features_df['vote_count'] >= 1000).astype(int)\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Apply feature engineering\n",
    "ml_features = engineer_features(ml_data_clean)\n",
    "\n",
    "# Select numeric features for training\n",
    "numeric_features = ['year_released', 'decade', 'is_recent', 'is_classic', 'runtime', 'is_short', 'is_long']\n",
    "\n",
    "# Add genre features if available\n",
    "if 'genres' in ml_features.columns:\n",
    "    genre_features = [col for col in ml_features.columns if col.startswith('is_') and 'genre' not in col.lower()]\n",
    "    numeric_features.extend(genre_features)\n",
    "\n",
    "# Add popularity features if available\n",
    "if 'vote_average' in ml_features.columns:\n",
    "    numeric_features.extend(['vote_average', 'is_highly_rated'])\n",
    "if 'vote_count' in ml_features.columns:  \n",
    "    numeric_features.extend(['vote_count', 'is_popular'])\n",
    "\n",
    "# Remove features that don't exist\n",
    "numeric_features = [col for col in numeric_features if col in ml_features.columns]\n",
    "\n",
    "print(f\"Engineered features: {numeric_features}\")\n",
    "\n",
    "# Prepare final training data\n",
    "X = ml_features[numeric_features].fillna(0)\n",
    "y = ml_features['Rating']\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts().sort_index())\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"Missing values in features: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in target: {y.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Training multiple ML models on your rating preferences...\n",
      "\n",
      "📊 Training Random Forest...\n",
      "  MSE: 0.8195\n",
      "  MAE: 0.7141\n",
      "  RMSE: 0.9053\n",
      "  CV MSE: 1.0186\n",
      "\n",
      "📊 Training Gradient Boosting...\n",
      "  MSE: 0.8989\n",
      "  MAE: 0.7481\n",
      "  RMSE: 0.9481\n",
      "  CV MSE: 1.4602\n",
      "\n",
      "📊 Training Ridge Regression...\n",
      "  MSE: 0.6768\n",
      "  MAE: 0.6740\n",
      "  RMSE: 0.8227\n",
      "  CV MSE: 0.8454\n",
      "\n",
      "🏆 Best model: Ridge Regression\n",
      "Best model CV RMSE: 0.9194\n",
      "\n",
      "📈 Top coefficients for Ridge Regression:\n",
      "  decade: -0.3151\n",
      "  is_classic: -0.3000\n",
      "  is_long: 0.1980\n",
      "  runtime: -0.0993\n",
      "  year_released: -0.0781\n"
     ]
    }
   ],
   "source": [
    "# Train Multiple ML Models\n",
    "print(\"🎯 Training multiple ML models on your rating preferences...\")\n",
    "\n",
    "# Split data for training and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models to try\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=6),\n",
    "    'Ridge Regression': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_performance = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n📊 Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for Ridge, original for tree-based models\n",
    "    if name == 'Ridge Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Cross validation on scaled data\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Cross validation on original data  \n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    cv_mse = -cv_scores.mean()\n",
    "    \n",
    "    model_performance[name] = {\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'CV_MSE': cv_mse,\n",
    "        'RMSE': np.sqrt(mse)\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")  \n",
    "    print(f\"  RMSE: {np.sqrt(mse):.4f}\")\n",
    "    print(f\"  CV MSE: {cv_mse:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = min(model_performance.keys(), key=lambda k: model_performance[k]['CV_MSE'])\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\n🏆 Best model: {best_model_name}\")\n",
    "print(f\"Best model CV RMSE: {np.sqrt(model_performance[best_model_name]['CV_MSE']):.4f}\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': numeric_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📈 Feature importance for {best_model_name}:\")\n",
    "    for _, row in feature_importance.head().iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "else:\n",
    "    # For Ridge regression, show coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': numeric_features,\n",
    "        'coefficient': best_model.coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\n📈 Top coefficients for {best_model_name}:\")\n",
    "    for _, row in coefficients.head().iterrows():\n",
    "        print(f\"  {row['feature']}: {row['coefficient']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Generating ML-based movie recommendations...\n",
      "Movies available for ML prediction: 277403\n",
      "🎯 Found 79899 movies predicted to be 4.0+ stars\n",
      "Top 50 ML-based recommendations:\n",
      " 1. Passage of Venus (1874) - Predicted Rating: 5.95/5.0\n",
      " 2. The Musician Monkey (1878) - Predicted Rating: 5.93/5.0\n",
      " 3. Sallie Gardner at a Gallop (1878) - Predicted Rating: 5.93/5.0\n",
      " 4. Le Rotisseur (1878) - Predicted Rating: 5.93/5.0\n",
      " 5. Les Bulles de Savon (1878) - Predicted Rating: 5.93/5.0\n",
      " 6. L'Équilibriste (1878) - Predicted Rating: 5.93/5.0\n",
      " 7. Le Jongleur (1878) - Predicted Rating: 5.93/5.0\n",
      " 8. The Aquarium (1878) - Predicted Rating: 5.93/5.0\n",
      " 9. Les Chiens Savants (1878) - Predicted Rating: 5.93/5.0\n",
      "10. Le Steeple-chase (1878) - Predicted Rating: 5.93/5.0\n",
      "11. La Nageuse (1878) - Predicted Rating: 5.93/5.0\n",
      "12. Le Repas des Poulets (1878) - Predicted Rating: 5.93/5.0\n",
      "13. Le Fumeur (1878) - Predicted Rating: 5.93/5.0\n",
      "14. La Balançoire (1878) - Predicted Rating: 5.93/5.0\n",
      "15. La Charmeuse (1878) - Predicted Rating: 5.93/5.0\n",
      "16. Le Trapèze (1878) - Predicted Rating: 5.93/5.0\n",
      "17. Les Papillons (1878) - Predicted Rating: 5.93/5.0\n",
      "18. Le Jeu du Volant (1878) - Predicted Rating: 5.93/5.0\n",
      "19. Zim, Boum, Boum (1878) - Predicted Rating: 5.93/5.0\n",
      "20. Le Jeu de Corde (1878) - Predicted Rating: 5.93/5.0\n",
      "\n",
      "📊 Recommendation Comparison:\n",
      "   • Overlap between ML and Collaborative: 0 movies\n",
      "   • Unique to ML recommendations: 49 movies\n",
      "   • Unique to Collaborative filtering: 0 movies\n",
      "✅ Saved 50 ML recommendations to 'data/movie_recommendations_ML_based.txt'\n"
     ]
    }
   ],
   "source": [
    "# Generate ML-based Movie Recommendations\n",
    "print(\"🤖 Generating ML-based movie recommendations...\")\n",
    "\n",
    "# Prepare all movies for prediction (excluding watched ones)\n",
    "all_movies_features = engineer_features(movies_clean_filtered)\n",
    "\n",
    "# Get movies you haven't watched\n",
    "unwatched_movies = all_movies_features[~all_movies_features['tmdb_id'].isin(my_watched_movies_set)].copy()\n",
    "print(f\"Movies available for ML prediction: {len(unwatched_movies)}\")\n",
    "\n",
    "# Prepare features for prediction\n",
    "unwatched_features = unwatched_movies[numeric_features].fillna(0)\n",
    "\n",
    "# Make predictions using the best model\n",
    "if best_model_name == 'Ridge Regression':\n",
    "    unwatched_features_scaled = scaler.transform(unwatched_features)\n",
    "    predicted_ratings = best_model.predict(unwatched_features_scaled)\n",
    "else:\n",
    "    predicted_ratings = best_model.predict(unwatched_features)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "unwatched_movies['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Filter for high predicted ratings (4.0+ stars)\n",
    "high_rated_predictions = unwatched_movies[unwatched_movies['predicted_rating'] >= 4.0].copy()\n",
    "\n",
    "# Sort by predicted rating and get top recommendations\n",
    "ml_recommendations = high_rated_predictions.nlargest(50, 'predicted_rating')\n",
    "\n",
    "print(f\"🎯 Found {len(high_rated_predictions)} movies predicted to be 4.0+ stars\")\n",
    "print(f\"Top 50 ML-based recommendations:\")\n",
    "\n",
    "# Display top ML recommendations\n",
    "ml_final_recommendations = []\n",
    "for i, (_, movie) in enumerate(ml_recommendations.iterrows(), 1):\n",
    "    title = movie['movie_title'] \n",
    "    year = int(movie['year_released']) if pd.notna(movie['year_released']) else 'Unknown'\n",
    "    pred_rating = movie['predicted_rating']\n",
    "    \n",
    "    ml_final_recommendations.append({\n",
    "        'rank': i,\n",
    "        'title': title,\n",
    "        'year': year,\n",
    "        'predicted_rating': pred_rating,\n",
    "        'tmdb_id': movie['tmdb_id']\n",
    "    })\n",
    "    \n",
    "    if i <= 20:  # Show top 20\n",
    "        print(f\"{i:2d}. {title} ({year}) - Predicted Rating: {pred_rating:.2f}/5.0\")\n",
    "\n",
    "# Compare with collaborative filtering recommendations\n",
    "collaborative_tmdb_ids = set(movie.get('tmdb_id') for movie in final_recommendations if 'tmdb_id' in movie)\n",
    "ml_tmdb_ids = set(ml_final_recommendations[i]['tmdb_id'] for i in range(len(ml_final_recommendations)))\n",
    "\n",
    "# Find overlap and unique recommendations\n",
    "overlap = len(collaborative_tmdb_ids.intersection(ml_tmdb_ids))\n",
    "ml_unique = len(ml_tmdb_ids - collaborative_tmdb_ids)\n",
    "collaborative_unique = len(collaborative_tmdb_ids - ml_tmdb_ids)\n",
    "\n",
    "print(f\"\\n📊 Recommendation Comparison:\")\n",
    "print(f\"   • Overlap between ML and Collaborative: {overlap} movies\")\n",
    "print(f\"   • Unique to ML recommendations: {ml_unique} movies\")\n",
    "print(f\"   • Unique to Collaborative filtering: {collaborative_unique} movies\")\n",
    "\n",
    "# Save ML recommendations\n",
    "ml_filename = 'data/movie_recommendations_ML_based.txt'\n",
    "with open(ml_filename, 'w') as f:\n",
    "    f.write(\"🤖 ML-BASED MOVIE RECOMMENDATIONS\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"Model: {best_model_name} (RMSE: {np.sqrt(model_performance[best_model_name]['CV_MSE']):.3f})\\n\")\n",
    "    f.write(f\"Based on {len(ml_features)} of your rated movies\\n\")\n",
    "    f.write(f\"Features used: {', '.join(numeric_features)}\\n\")\n",
    "    f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(\"🎯 Movies predicted to match your taste:\\n\\n\")\n",
    "    \n",
    "    for movie in ml_final_recommendations:\n",
    "        f.write(f\"{movie['rank']:2d}. {movie['title']} ({movie['year']})\\n\")\n",
    "        f.write(f\"    🤖 Predicted Rating: {movie['predicted_rating']:.2f}/5.0\\n\\n\")\n",
    "\n",
    "print(f\"✅ Saved {len(ml_final_recommendations)} ML recommendations to '{ml_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Creating balanced ML recommendations and hybrid system...\n",
      "\n",
      "🎬 Top 20 Modern ML Recommendations (1980+):\n",
      " 1. The Bunker (1981) - Predicted: 4.70/5.0\n",
      " 2. Jacqueline Bouvier Kennedy (1981) - Predicted: 4.70/5.0\n",
      " 3. Miracle on Ice (1981) - Predicted: 4.70/5.0\n",
      " 4. Das Boot (1981) - Predicted: 4.70/5.0\n",
      " 5. Of Mice and Men (1981) - Predicted: 4.70/5.0\n",
      " 6. The Chelsea Murders (1981) - Predicted: 4.70/5.0\n",
      " 7. Netrikan (1981) - Predicted: 4.70/5.0\n",
      " 8. Midnight (1981) - Predicted: 4.70/5.0\n",
      " 9. Jibon Nouka (1981) - Predicted: 4.70/5.0\n",
      "10. Why Not? (1981) - Predicted: 4.70/5.0\n",
      "11. Zoombie (1982) - Predicted: 4.69/5.0\n",
      "12. Camelot (1982) - Predicted: 4.69/5.0\n",
      "13. Kalyug (1981) - Predicted: 4.69/5.0\n",
      "14. Schöne Tage (1981) - Predicted: 4.69/5.0\n",
      "15. The Night of Varennes (1982) - Predicted: 4.69/5.0\n",
      "16. Threshold (1982) - Predicted: 4.69/5.0\n",
      "17. Sophie's Choice (1982) - Predicted: 4.69/5.0\n",
      "18. The Victor (1982) - Predicted: 4.69/5.0\n",
      "19. Carnival (1982) - Predicted: 4.69/5.0\n",
      "20. The Uppercrust (1981) - Predicted: 4.69/5.0\n",
      "\n",
      "🤝 Creating Hybrid Recommendations...\n",
      "\n",
      "🏆 Top 20 Hybrid Recommendations:\n",
      " 1. The Bunker (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 2. Jacqueline Bouvier Kennedy (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 3. Miracle on Ice (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 4. Das Boot (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 5. Of Mice and Men (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 6. The Chelsea Murders (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 7. Netrikan (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 8. Midnight (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      " 9. Jibon Nouka (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      "10. Why Not? (1981) - ML Only\n",
      "    ML Score: 4.70\n",
      "\n",
      "11. Zoombie (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "12. Camelot (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "13. Kalyug (1981) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "14. Schöne Tage (1981) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "15. The Night of Varennes (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "16. Threshold (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "17. Sophie's Choice (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "18. The Victor (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "19. Carnival (1982) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "20. The Uppercrust (1981) - ML Only\n",
      "    ML Score: 4.69\n",
      "\n",
      "✅ Saved recommendations to:\n",
      "   • data/movie_recommendations_ML_modern.txt\n",
      "   • data/movie_recommendations_HYBRID.txt\n",
      "\n",
      "📊 Final Summary:\n",
      "   • Collaborative Filtering: 50 recommendations\n",
      "   • Modern ML Predictions: 50 recommendations\n",
      "   • Hybrid Approach: 30 recommendations\n",
      "   • Both approaches complement each other for comprehensive coverage!\n"
     ]
    }
   ],
   "source": [
    "# Create Balanced ML Recommendations and Hybrid System\n",
    "print(\"🎯 Creating balanced ML recommendations and hybrid system...\")\n",
    "\n",
    "# Filter for more modern movies to get practical recommendations\n",
    "modern_unwatched = unwatched_movies[unwatched_movies['year_released'] >= 1980].copy()\n",
    "modern_unwatched['predicted_rating'] = modern_unwatched['predicted_rating']\n",
    "\n",
    "# Get top modern ML recommendations\n",
    "modern_ml_recs = modern_unwatched.nlargest(50, 'predicted_rating')\n",
    "\n",
    "print(f\"\\n🎬 Top 20 Modern ML Recommendations (1980+):\")\n",
    "modern_ml_final = []\n",
    "for i, (_, movie) in enumerate(modern_ml_recs.iterrows(), 1):\n",
    "    title = movie['movie_title'] \n",
    "    year = int(movie['year_released']) if pd.notna(movie['year_released']) else 'Unknown'\n",
    "    pred_rating = movie['predicted_rating']\n",
    "    \n",
    "    modern_ml_final.append({\n",
    "        'rank': i,\n",
    "        'title': title,\n",
    "        'year': year,\n",
    "        'predicted_rating': pred_rating,\n",
    "        'tmdb_id': movie['tmdb_id'],\n",
    "        'source': 'ML'\n",
    "    })\n",
    "    \n",
    "    if i <= 20:\n",
    "        print(f\"{i:2d}. {title} ({year}) - Predicted: {pred_rating:.2f}/5.0\")\n",
    "\n",
    "# Create Hybrid Recommendations (combine both approaches)\n",
    "print(f\"\\n🤝 Creating Hybrid Recommendations...\")\n",
    "\n",
    "# Prepare collaborative filtering scores for comparison\n",
    "collaborative_scores = {}\n",
    "for movie in final_recommendations:\n",
    "    collaborative_scores[movie.get('tmdb_id')] = {\n",
    "        'title': movie['title'],\n",
    "        'year': movie['year'], \n",
    "        'collaborative_score': movie['combined_score'],\n",
    "        'avg_rating': movie['avg_rating'],\n",
    "        'source': 'Collaborative'\n",
    "    }\n",
    "\n",
    "# Prepare ML scores for modern movies\n",
    "ml_scores = {}\n",
    "for movie in modern_ml_final:\n",
    "    ml_scores[movie['tmdb_id']] = {\n",
    "        'title': movie['title'],\n",
    "        'year': movie['year'],\n",
    "        'ml_score': movie['predicted_rating'],\n",
    "        'source': 'ML'\n",
    "    }\n",
    "\n",
    "# Create hybrid scoring\n",
    "hybrid_recommendations = []\n",
    "\n",
    "# Add collaborative filtering movies with hybrid scores\n",
    "for tmdb_id, collab_data in collaborative_scores.items():\n",
    "    if tmdb_id in ml_scores:\n",
    "        # Movie appears in both systems - combine scores\n",
    "        ml_data = ml_scores[tmdb_id]\n",
    "        hybrid_score = (collab_data['collaborative_score'] / 100) + (ml_data['ml_score'])  # Normalize and combine\n",
    "        source = 'Hybrid'\n",
    "    else:\n",
    "        # Only in collaborative filtering\n",
    "        hybrid_score = collab_data['collaborative_score'] / 100\n",
    "        source = 'Collaborative Only'\n",
    "    \n",
    "    hybrid_recommendations.append({\n",
    "        'tmdb_id': tmdb_id,\n",
    "        'title': collab_data['title'],\n",
    "        'year': collab_data['year'],\n",
    "        'hybrid_score': hybrid_score,\n",
    "        'source': source,\n",
    "        'collaborative_score': collab_data.get('collaborative_score', 0),\n",
    "        'ml_score': ml_scores.get(tmdb_id, {}).get('ml_score', 0)\n",
    "    })\n",
    "\n",
    "# Add ML-only movies\n",
    "for tmdb_id, ml_data in ml_scores.items():\n",
    "    if tmdb_id not in collaborative_scores:\n",
    "        hybrid_recommendations.append({\n",
    "            'tmdb_id': tmdb_id,\n",
    "            'title': ml_data['title'],\n",
    "            'year': ml_data['year'],\n",
    "            'hybrid_score': ml_data['ml_score'],\n",
    "            'source': 'ML Only',\n",
    "            'collaborative_score': 0,\n",
    "            'ml_score': ml_data['ml_score']\n",
    "        })\n",
    "\n",
    "# Sort hybrid recommendations by score\n",
    "hybrid_recommendations.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "top_hybrid = hybrid_recommendations[:30]\n",
    "\n",
    "print(f\"\\n🏆 Top 20 Hybrid Recommendations:\")\n",
    "for i, movie in enumerate(top_hybrid[:20], 1):\n",
    "    print(f\"{i:2d}. {movie['title']} ({movie['year']}) - {movie['source']}\")\n",
    "    if movie['source'] == 'Hybrid':\n",
    "        print(f\"    Hybrid Score: {movie['hybrid_score']:.2f} (ML: {movie['ml_score']:.2f}, Collab: {movie['collaborative_score']:.0f})\")\n",
    "    elif movie['source'] == 'Collaborative Only':\n",
    "        print(f\"    Collaborative Score: {movie['collaborative_score']:.0f}\")\n",
    "    else:\n",
    "        print(f\"    ML Score: {movie['ml_score']:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Save all three recommendation types\n",
    "files_saved = []\n",
    "\n",
    "# Modern ML recommendations\n",
    "modern_ml_filename = 'data/movie_recommendations_ML_modern.txt'\n",
    "with open(modern_ml_filename, 'w') as f:\n",
    "    f.write(\"🎬 MODERN ML-BASED MOVIE RECOMMENDATIONS (1980+)\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Based on analysis of your rating patterns\\n\")\n",
    "    f.write(f\"Filtered for movies from 1980 onwards\\n\")\n",
    "    f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    for movie in modern_ml_final:\n",
    "        f.write(f\"{movie['rank']:2d}. {movie['title']} ({movie['year']})\\n\")\n",
    "        f.write(f\"    🤖 Predicted Rating: {movie['predicted_rating']:.2f}/5.0\\n\\n\")\n",
    "\n",
    "files_saved.append(modern_ml_filename)\n",
    "\n",
    "# Hybrid recommendations\n",
    "hybrid_filename = 'data/movie_recommendations_HYBRID.txt'\n",
    "with open(hybrid_filename, 'w') as f:\n",
    "    f.write(\"🤝 HYBRID MOVIE RECOMMENDATIONS\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    f.write(\"Combines Machine Learning + Collaborative Filtering\\n\")\n",
    "    f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    for i, movie in enumerate(top_hybrid, 1):\n",
    "        f.write(f\"{i:2d}. {movie['title']} ({movie['year']}) - {movie['source']}\\n\")\n",
    "        if movie['source'] == 'Hybrid':\n",
    "            f.write(f\"    🤝 Hybrid Score: {movie['hybrid_score']:.2f}\\n\")\n",
    "            f.write(f\"    🤖 ML Score: {movie['ml_score']:.2f}\\n\")\n",
    "            f.write(f\"    👥 Collaborative Score: {movie['collaborative_score']:.0f}\\n\")\n",
    "        elif movie['source'] == 'Collaborative Only':\n",
    "            f.write(f\"    👥 Collaborative Score: {movie['collaborative_score']:.0f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"    🤖 ML Score: {movie['ml_score']:.2f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "files_saved.append(hybrid_filename)\n",
    "\n",
    "print(f\"✅ Saved recommendations to:\")\n",
    "for filename in files_saved:\n",
    "    print(f\"   • {filename}\")\n",
    "\n",
    "print(f\"\\n📊 Final Summary:\")\n",
    "print(f\"   • Collaborative Filtering: {len(final_recommendations)} recommendations\")\n",
    "print(f\"   • Modern ML Predictions: {len(modern_ml_final)} recommendations\")  \n",
    "print(f\"   • Hybrid Approach: {len(top_hybrid)} recommendations\")\n",
    "print(f\"   • Both approaches complement each other for comprehensive coverage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating OVERFITTED ML model with genre information...\n",
      "🎭 Analyzing genre information...\n",
      "Available genre columns: ['genres']\n",
      "Sample genre data:\n",
      "  [\"Music\",\"Animation\"]\n",
      "  []\n",
      "  [\"Drama\"]\n",
      "  [\"Drama\"]\n",
      "  [\"Documentary\"]\n",
      "  [\"Romance\"]\n",
      "  [\"Drama\",\"Thriller\"]\n",
      "  []\n",
      "  [\"Music\",\"Documentary\"]\n",
      "  [\"Drama\"]\n",
      "🔧 Creating overfitted features...\n",
      "Total engineered features: 64\n",
      "Sample features: ['year_released', 'runtime', 'decade_1920s', 'decade_1930s', 'decade_1940s', 'decade_1950s', 'decade_1960s', 'decade_1970s', 'decade_1980s', 'decade_1990s']...\n",
      "Overfitted training data shape: (198, 64)\n",
      "Features with non-zero variance: 62\n",
      "Final feature set: 62 features\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED ML MODEL WITH GENRE INFO AND OVERFITTING\n",
    "print(\"🚀 Creating OVERFITTED ML model with genre information...\")\n",
    "\n",
    "# First, let's explore genre information in the dataset\n",
    "print(\"🎭 Analyzing genre information...\")\n",
    "genre_cols = [col for col in movies_clean_filtered.columns if 'genre' in col.lower()]\n",
    "print(f\"Available genre columns: {genre_cols}\")\n",
    "\n",
    "# Check what genre data we have\n",
    "if 'genres' in movies_clean_filtered.columns:\n",
    "    print(\"Sample genre data:\")\n",
    "    sample_genres = movies_clean_filtered[movies_clean_filtered['genres'].notna()]['genres'].head(10)\n",
    "    for i, genres in enumerate(sample_genres):\n",
    "        print(f\"  {genres}\")\n",
    "elif 'genre' in movies_clean_filtered.columns:\n",
    "    print(\"Sample genre data:\")\n",
    "    sample_genres = movies_clean_filtered[movies_clean_filtered['genre'].notna()]['genre'].head(10)\n",
    "    for i, genre in enumerate(sample_genres):\n",
    "        print(f\"  {genre}\")\n",
    "\n",
    "# Enhanced feature engineering with extensive genre features\n",
    "def engineer_overfitted_features(df, your_ratings=None):\n",
    "    \"\"\"Engineer LOTS of features to overfit to your specific taste\"\"\"\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # Year features (more granular)\n",
    "    if 'year_released' in features_df.columns:\n",
    "        features_df['year_released'] = pd.to_numeric(features_df['year_released'], errors='coerce').fillna(1950)\n",
    "        \n",
    "        # Decade features\n",
    "        for decade in range(1920, 2030, 10):\n",
    "            features_df[f'decade_{decade}s'] = (features_df['year_released'].between(decade, decade+9)).astype(int)\n",
    "        \n",
    "        # Era features\n",
    "        features_df['is_silent_era'] = (features_df['year_released'] <= 1929).astype(int)\n",
    "        features_df['is_golden_age'] = features_df['year_released'].between(1930, 1959).astype(int)\n",
    "        features_df['is_new_hollywood'] = features_df['year_released'].between(1960, 1979).astype(int)\n",
    "        features_df['is_blockbuster_era'] = features_df['year_released'].between(1980, 1999).astype(int)\n",
    "        features_df['is_digital_era'] = features_df['year_released'].between(2000, 2019).astype(int)\n",
    "        features_df['is_streaming_era'] = (features_df['year_released'] >= 2020).astype(int)\n",
    "        \n",
    "        # Age features\n",
    "        current_year = 2025\n",
    "        features_df['movie_age'] = current_year - features_df['year_released']\n",
    "        features_df['is_very_old'] = (features_df['movie_age'] > 50).astype(int)\n",
    "        features_df['is_classic'] = features_df['movie_age'].between(25, 50).astype(int)\n",
    "        features_df['is_modern'] = features_df['movie_age'].between(10, 25).astype(int)\n",
    "        features_df['is_recent'] = (features_df['movie_age'] <= 10).astype(int)\n",
    "    \n",
    "    # Runtime features (more granular)\n",
    "    if 'runtime' in features_df.columns:\n",
    "        features_df['runtime'] = pd.to_numeric(features_df['runtime'], errors='coerce').fillna(90)\n",
    "        \n",
    "        # Length categories\n",
    "        features_df['is_short'] = (features_df['runtime'] <= 90).astype(int)\n",
    "        features_df['is_medium'] = features_df['runtime'].between(91, 130).astype(int)\n",
    "        features_df['is_long'] = features_df['runtime'].between(131, 180).astype(int)\n",
    "        features_df['is_epic'] = (features_df['runtime'] > 180).astype(int)\n",
    "        \n",
    "        # Specific runtime preferences\n",
    "        for length in [80, 90, 100, 110, 120, 140, 160]:\n",
    "            features_df[f'runtime_around_{length}'] = (abs(features_df['runtime'] - length) <= 10).astype(int)\n",
    "    \n",
    "    # EXTENSIVE Genre features\n",
    "    genre_col = None\n",
    "    if 'genres' in features_df.columns and features_df['genres'].notna().any():\n",
    "        genre_col = 'genres'\n",
    "    elif 'genre' in features_df.columns and features_df['genre'].notna().any():\n",
    "        genre_col = 'genre'\n",
    "    \n",
    "    if genre_col:\n",
    "        features_df['genre_string'] = features_df[genre_col].fillna('')\n",
    "        \n",
    "        # Major genres\n",
    "        major_genres = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', \n",
    "                       'Drama', 'Family', 'Fantasy', 'Horror', 'Music', 'Mystery', \n",
    "                       'Romance', 'Science Fiction', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "        \n",
    "        for genre in major_genres:\n",
    "            col_name = f'genre_{genre.lower().replace(\" \", \"_\").replace(\"-\", \"_\")}'\n",
    "            features_df[col_name] = features_df['genre_string'].str.contains(genre, case=False, na=False).astype(int)\n",
    "        \n",
    "        # Genre combinations (your specific preferences)\n",
    "        features_df['is_action_adventure'] = ((features_df.get('genre_action', 0) == 1) & \n",
    "                                            (features_df.get('genre_adventure', 0) == 1)).astype(int)\n",
    "        features_df['is_drama_romance'] = ((features_df.get('genre_drama', 0) == 1) & \n",
    "                                         (features_df.get('genre_romance', 0) == 1)).astype(int)\n",
    "        features_df['is_comedy_romance'] = ((features_df.get('genre_comedy', 0) == 1) & \n",
    "                                          (features_df.get('genre_romance', 0) == 1)).astype(int)\n",
    "        features_df['is_sci_fi_action'] = ((features_df.get('genre_science_fiction', 0) == 1) & \n",
    "                                         (features_df.get('genre_action', 0) == 1)).astype(int)\n",
    "        \n",
    "        # Count genres (diversity measure)\n",
    "        genre_count_cols = [col for col in features_df.columns if col.startswith('genre_') and col != 'genre_string']\n",
    "        if genre_count_cols:\n",
    "            features_df['genre_count'] = features_df[genre_count_cols].sum(axis=1)\n",
    "            features_df['is_single_genre'] = (features_df['genre_count'] == 1).astype(int)\n",
    "            features_df['is_multi_genre'] = (features_df['genre_count'] > 1).astype(int)\n",
    "    \n",
    "    # Rating-based features (if available)\n",
    "    if 'vote_average' in features_df.columns:\n",
    "        features_df['vote_average'] = pd.to_numeric(features_df['vote_average'], errors='coerce').fillna(5.0)\n",
    "        features_df['is_highly_rated'] = (features_df['vote_average'] >= 7.5).astype(int)\n",
    "        features_df['is_critically_acclaimed'] = (features_df['vote_average'] >= 8.0).astype(int)\n",
    "        features_df['is_poor_rated'] = (features_df['vote_average'] <= 5.0).astype(int)\n",
    "        \n",
    "        # Rating ranges\n",
    "        for rating in [6.0, 6.5, 7.0, 7.5, 8.0, 8.5]:\n",
    "            features_df[f'rating_around_{rating:.1f}'] = (abs(features_df['vote_average'] - rating) <= 0.25).astype(int)\n",
    "    \n",
    "    if 'vote_count' in features_df.columns:\n",
    "        features_df['vote_count'] = pd.to_numeric(features_df['vote_count'], errors='coerce').fillna(100)\n",
    "        features_df['log_vote_count'] = np.log1p(features_df['vote_count'])\n",
    "        features_df['is_popular'] = (features_df['vote_count'] >= 1000).astype(int)\n",
    "        features_df['is_very_popular'] = (features_df['vote_count'] >= 10000).astype(int)\n",
    "        features_df['is_niche'] = (features_df['vote_count'] <= 100).astype(int)\n",
    "    \n",
    "    # If we have your ratings, create personalized features\n",
    "    if your_ratings is not None:\n",
    "        # Your preference patterns\n",
    "        your_avg = your_ratings['Rating'].mean()\n",
    "        your_std = your_ratings['Rating'].std()\n",
    "        \n",
    "        # Add features based on your rating patterns\n",
    "        if 'year_released' in features_df.columns and 'year_released' in your_ratings.columns:\n",
    "            # Years you tend to rate highly\n",
    "            high_rated_years = your_ratings[your_ratings['Rating'] >= 4.5]['year_released'].tolist()\n",
    "            for year in high_rated_years:\n",
    "                features_df[f'your_fav_year_{int(year)}'] = (features_df['year_released'] == year).astype(int)\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Apply overfitted feature engineering\n",
    "print(\"🔧 Creating overfitted features...\")\n",
    "ml_data_overfitted = engineer_overfitted_features(ml_data_clean, ml_data_clean)\n",
    "\n",
    "# Get all engineered features (excluding non-numeric columns)\n",
    "exclude_cols = ['tmdb_id', 'movie_title', 'Rating', 'genre_string', 'user_id', 'genres']\n",
    "feature_cols = [col for col in ml_data_overfitted.columns if col not in exclude_cols]\n",
    "\n",
    "# Only keep numeric features\n",
    "numeric_feature_cols = []\n",
    "for col in feature_cols:\n",
    "    try:\n",
    "        pd.to_numeric(ml_data_overfitted[col], errors='raise')\n",
    "        numeric_feature_cols.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "\n",
    "print(f\"Total engineered features: {len(numeric_feature_cols)}\")\n",
    "print(f\"Sample features: {numeric_feature_cols[:10]}...\")\n",
    "\n",
    "# Prepare training data\n",
    "X_overfitted = ml_data_overfitted[numeric_feature_cols].fillna(0)\n",
    "y_overfitted = ml_data_overfitted['Rating']\n",
    "\n",
    "print(f\"Overfitted training data shape: {X_overfitted.shape}\")\n",
    "print(f\"Features with non-zero variance: {(X_overfitted.var() > 0).sum()}\")\n",
    "\n",
    "# Remove features with zero variance\n",
    "non_zero_variance_cols = X_overfitted.columns[X_overfitted.var() > 0]\n",
    "X_overfitted_clean = X_overfitted[non_zero_variance_cols]\n",
    "\n",
    "print(f\"Final feature set: {X_overfitted_clean.shape[1]} features\")\n",
    "\n",
    "# Split data\n",
    "X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(\n",
    "    X_overfitted_clean, y_overfitted, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_overfitted = StandardScaler()\n",
    "X_train_over_scaled = scaler_overfitted.fit_transform(X_train_over)\n",
    "X_test_over_scaled = scaler_overfitted.transform(X_test_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Training OVERFITTED models to learn your specific taste...\n",
      "\n",
      "🤖 Training Random Forest (Overfitted)...\n",
      "   Train RMSE: 0.4486\n",
      "   Test RMSE: 0.8450\n",
      "   CV RMSE: 1.0129\n",
      "   Overfitting Ratio: 0.282 (lower = more overfitted)\n",
      "\n",
      "🤖 Training Gradient Boosting (Overfitted)...\n",
      "   Train RMSE: 0.3023\n",
      "   Test RMSE: 0.9706\n",
      "   CV RMSE: 1.1541\n",
      "   Overfitting Ratio: 0.097 (lower = more overfitted)\n",
      "\n",
      "🤖 Training Ridge (High Complexity)...\n",
      "   Train RMSE: 0.7617\n",
      "   Test RMSE: 0.9599\n",
      "   CV RMSE: 1.0840\n",
      "   Overfitting Ratio: 0.630 (lower = more overfitted)\n",
      "\n",
      "🏆 OVERFITTED MODEL COMPARISON:\n",
      "================================================================================\n",
      "Random Forest (Overfitted) | CV RMSE: 1.0129 | Overfit: 0.282\n",
      "Gradient Boosting (Overfitted) | CV RMSE: 1.1541 | Overfit: 0.097\n",
      "Ridge (High Complexity)   | CV RMSE: 1.0840 | Overfit: 0.630\n",
      "\n",
      "🎯 Selected OVERFITTED model: Random Forest (Overfitted)\n",
      "   Cross-validation RMSE: 1.0129\n",
      "   Overfitting ratio: 0.282\n",
      "\n",
      "🔍 TOP 15 MOST IMPORTANT FEATURES (for your taste):\n",
      "    1. runtime                   | Importance: 0.2422\n",
      "    2. movie_age                 | Importance: 0.1075\n",
      "    3. year_released             | Importance: 0.1015\n",
      "    4. runtime_around_110        | Importance: 0.0461\n",
      "    5. runtime_around_100        | Importance: 0.0417\n",
      "    6. your_fav_year_2017        | Importance: 0.0268\n",
      "    7. runtime_around_120        | Importance: 0.0252\n",
      "    8. is_recent                 | Importance: 0.0227\n",
      "    9. your_fav_year_2013        | Importance: 0.0204\n",
      "   10. runtime_around_160        | Importance: 0.0203\n",
      "   11. runtime_around_140        | Importance: 0.0197\n",
      "   12. is_medium                 | Importance: 0.0174\n",
      "   13. is_modern                 | Importance: 0.0171\n",
      "   14. runtime_around_90         | Importance: 0.0160\n",
      "   15. your_fav_year_2015        | Importance: 0.0158\n"
     ]
    }
   ],
   "source": [
    "# Train OVERFITTED Models (designed to memorize your preferences)\n",
    "print(\"🧠 Training OVERFITTED models to learn your specific taste...\")\n",
    "\n",
    "# Multiple overfitted models\n",
    "overfitted_models = {\n",
    "    'Random Forest (Overfitted)': RandomForestRegressor(\n",
    "        n_estimators=500,          # Many trees\n",
    "        max_depth=None,            # No depth limit (overfit!)\n",
    "        min_samples_split=2,       # Minimal splits (overfit!)\n",
    "        min_samples_leaf=1,        # Minimal leaf size (overfit!)\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Gradient Boosting (Overfitted)': GradientBoostingRegressor(\n",
    "        n_estimators=500,          # Many iterations\n",
    "        learning_rate=0.05,        # Lower rate, more iterations\n",
    "        max_depth=10,              # Deep trees\n",
    "        min_samples_split=2,       # Minimal splits\n",
    "        min_samples_leaf=1,        # Minimal leaf size\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Ridge (High Complexity)': Ridge(\n",
    "        alpha=0.01,                # Less regularization (more overfitting)\n",
    "        max_iter=10000\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate overfitted models\n",
    "overfitted_results = {}\n",
    "trained_overfitted_models = {}\n",
    "\n",
    "for name, model in overfitted_models.items():\n",
    "    print(f\"\\n🤖 Training {name}...\")\n",
    "    \n",
    "    if 'Ridge' in name:\n",
    "        # Use scaled features for Ridge\n",
    "        model.fit(X_train_over_scaled, y_train_over)\n",
    "        y_pred_train = model.predict(X_train_over_scaled)\n",
    "        y_pred_test = model.predict(X_test_over_scaled)\n",
    "        \n",
    "        # Cross-validation with scaled features\n",
    "        cv_scores = cross_val_score(model, X_overfitted_clean, y_overfitted, \n",
    "                                  cv=3, scoring='neg_mean_squared_error')\n",
    "        cv_mse = -cv_scores.mean()\n",
    "    else:\n",
    "        # Use unscaled features for tree-based models\n",
    "        model.fit(X_train_over, y_train_over)\n",
    "        y_pred_train = model.predict(X_train_over)\n",
    "        y_pred_test = model.predict(X_test_over)\n",
    "        \n",
    "        # Cross-validation with unscaled features  \n",
    "        cv_scores = cross_val_score(model, X_overfitted_clean, y_overfitted,\n",
    "                                  cv=3, scoring='neg_mean_squared_error')\n",
    "        cv_mse = -cv_scores.mean()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train_over, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test_over, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train_over, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test_over, y_pred_test)\n",
    "    \n",
    "    overfitted_results[name] = {\n",
    "        'Train_MSE': train_mse,\n",
    "        'Test_MSE': test_mse,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'CV_MSE': cv_mse,\n",
    "        'CV_RMSE': np.sqrt(cv_mse),\n",
    "        'Overfitting': train_mse / test_mse  # Lower = more overfitted (good for our purpose!)\n",
    "    }\n",
    "    \n",
    "    trained_overfitted_models[name] = model\n",
    "    \n",
    "    print(f\"   Train RMSE: {np.sqrt(train_mse):.4f}\")\n",
    "    print(f\"   Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "    print(f\"   CV RMSE: {np.sqrt(cv_mse):.4f}\")\n",
    "    print(f\"   Overfitting Ratio: {train_mse / test_mse:.3f} (lower = more overfitted)\")\n",
    "\n",
    "# Select best overfitted model (prioritize low CV error but allow overfitting)\n",
    "print(f\"\\n🏆 OVERFITTED MODEL COMPARISON:\")\n",
    "print(\"=\"*80)\n",
    "for name, metrics in overfitted_results.items():\n",
    "    print(f\"{name:25} | CV RMSE: {metrics['CV_RMSE']:.4f} | Overfit: {metrics['Overfitting']:.3f}\")\n",
    "\n",
    "# Choose model with lowest CV RMSE (we want overfitting to your specific taste)\n",
    "best_overfitted_name = min(overfitted_results.keys(), key=lambda x: overfitted_results[x]['CV_RMSE'])\n",
    "best_overfitted_model = trained_overfitted_models[best_overfitted_name]\n",
    "\n",
    "print(f\"\\n🎯 Selected OVERFITTED model: {best_overfitted_name}\")\n",
    "print(f\"   Cross-validation RMSE: {overfitted_results[best_overfitted_name]['CV_RMSE']:.4f}\")\n",
    "print(f\"   Overfitting ratio: {overfitted_results[best_overfitted_name]['Overfitting']:.3f}\")\n",
    "\n",
    "# Feature importance for overfitted model\n",
    "if hasattr(best_overfitted_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_overfitted_clean.columns,\n",
    "        'importance': best_overfitted_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n🔍 TOP 15 MOST IMPORTANT FEATURES (for your taste):\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
    "        print(f\"   {i+1:2d}. {row['feature']:25} | Importance: {row['importance']:.4f}\")\n",
    "\n",
    "elif hasattr(best_overfitted_model, 'coef_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_overfitted_clean.columns,\n",
    "        'coefficient': np.abs(best_overfitted_model.coef_)\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\n🔍 TOP 15 MOST IMPORTANT FEATURES (by coefficient magnitude):\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
    "        print(f\"   {i+1:2d}. {row['feature']:25} | |Coef|: {row['coefficient']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Generating OVERFITTED movie predictions with proper rating scale...\n",
      "🔧 Applying overfitted features to entire movie database...\n",
      "Unwatched movies for overfitted prediction: 277403\n",
      "Making predictions with Random Forest (Overfitted)...\n",
      "Raw prediction range: 1.72 to 4.70\n",
      "Clipped prediction range: 1.72 to 4.70\n",
      "🎯 Found 229692 movies predicted 3.5+ stars\n",
      "🎯 Found 85796 movies predicted 4.0+ stars\n",
      "🎯 Found 346 movies predicted 4.5+ stars\n",
      "\n",
      "🧠 TOP 25 OVERFITTED ML RECOMMENDATIONS:\n",
      " 1. The Saint (1997) - 4.70/5.0 | [\"Thriller\",\"Action\",\"Romance\",\"Science Fiction\",\"...\n",
      " 2. Ashes of Paradise (1997) - 4.70/5.0 | [\"Crime\",\"Drama\",\"Mystery\",\"Thriller\"]\n",
      " 3. Beat (1997) - 4.70/5.0 | [\"Action\",\"Drama\"]\n",
      " 4. Secrets of Madonna (1997) - 4.70/5.0 | [\"Horror\",\"Drama\"]\n",
      " 5. A Rifle for Sleeping (1997) - 4.70/5.0 | [\"Crime\",\"Thriller\"]\n",
      " 6. The Mill on the Floss (1997) - 4.70/5.0 | [\"Drama\",\"Romance\"]\n",
      " 7. Con Air (1997) - 4.70/5.0 | [\"Action\",\"Thriller\",\"Crime\"]\n",
      " 8. Run for Your Life (1997) - 4.70/5.0 | [\"Drama\",\"Thriller\"]\n",
      " 9. Birdheart Pie (1997) - 4.70/5.0 | []\n",
      "10. Balkan Rules (1997) - 4.70/5.0 | [\"Drama\"]\n",
      "11. Run for Your Life (1997) - 4.70/5.0 | [\"Drama\",\"Thriller\"]\n",
      "12. Open Your Eyes (1997) - 4.69/5.0 | [\"Drama\",\"Thriller\"]\n",
      "13. Napomuceno's Will (1997) - 4.69/5.0 | [\"Drama\"]\n",
      "14. 12 Angry Men (1997) - 4.69/5.0 | [\"Crime\",\"Drama\",\"TV Movie\"]\n",
      "15. Bella Mafia (1997) - 4.69/5.0 | [\"Drama\",\"Crime\"]\n",
      "16. The Edge (1997) - 4.69/5.0 | [\"Action\",\"Adventure\",\"Drama\"]\n",
      "17. Déjà Vu (1997) - 4.69/5.0 | [\"Romance\",\"Drama\"]\n",
      "18. The Eel (1997) - 4.69/5.0 | [\"Drama\"]\n",
      "19. Moonlight Serenade (1997) - 4.69/5.0 | [\"Drama\"]\n",
      "20. Armageddon (1997) - 4.69/5.0 | [\"Fantasy\",\"Action\",\"Comedy\"]\n",
      "21. Metro (1997) - 4.69/5.0 | [\"Action\",\"Adventure\",\"Comedy\",\"Crime\",\"Thriller\"]\n",
      "22. The Weight of Cotton (1997) - 4.69/5.0 | [\"Drama\"]\n",
      "23. Intimates (1997) - 4.69/5.0 | [\"Romance\",\"Drama\"]\n",
      "24. Rui Ka Bojh (1997) - 4.69/5.0 | [\"Drama\"]\n",
      "25. Snide and Prejudice (1997) - 4.68/5.0 | [\"Drama\"]\n",
      "\n",
      "📊 OVERFITTED PREDICTION ANALYSIS:\n",
      "Your actual rating distribution:\n",
      "   0.5 stars:   1 movies (0.5%)\n",
      "   1.0 stars:   4 movies (2.0%)\n",
      "   1.5 stars:   6 movies (3.0%)\n",
      "   2.0 stars:   9 movies (4.5%)\n",
      "   2.5 stars:   9 movies (4.5%)\n",
      "   3.0 stars:  16 movies (8.1%)\n",
      "   3.5 stars:  39 movies (19.7%)\n",
      "   4.0 stars:  68 movies (34.3%)\n",
      "   4.5 stars:  34 movies (17.2%)\n",
      "   5.0 stars:  12 movies (6.1%)\n",
      "\n",
      "Predicted rating distribution (all unwatched movies):\n",
      "   (1.5, 2.0]:   630 movies (0.2%)\n",
      "   (2.0, 2.5]:  1995 movies (0.7%)\n",
      "   (2.5, 3.0]: 10535 movies (3.8%)\n",
      "   (3.0, 3.5]: 34601 movies (12.5%)\n",
      "   (3.5, 4.0]: 143968 movies (51.9%)\n",
      "   (4.0, 4.5]: 85328 movies (30.8%)\n",
      "   (4.5, 5.0]:   346 movies (0.1%)\n",
      "✅ Saved 100 OVERFITTED recommendations to 'data/movie_recommendations_OVERFITTED_with_genres.txt'\n"
     ]
    }
   ],
   "source": [
    "# Generate OVERFITTED Predictions with Proper Rating Scale (0.5-5.0)\n",
    "print(\"🎬 Generating OVERFITTED movie predictions with proper rating scale...\")\n",
    "\n",
    "# Apply overfitted feature engineering to ALL movies\n",
    "print(\"🔧 Applying overfitted features to entire movie database...\")\n",
    "all_movies_overfitted = engineer_overfitted_features(movies_clean_filtered, ml_data_clean)\n",
    "\n",
    "# Filter for unwatched movies\n",
    "unwatched_overfitted = all_movies_overfitted[~all_movies_overfitted['tmdb_id'].isin(my_watched_movies_set)].copy()\n",
    "print(f\"Unwatched movies for overfitted prediction: {len(unwatched_overfitted)}\")\n",
    "\n",
    "# Prepare features (same as training)\n",
    "unwatched_features_over = unwatched_overfitted[non_zero_variance_cols].fillna(0)\n",
    "\n",
    "# Make predictions with overfitted model\n",
    "print(f\"Making predictions with {best_overfitted_name}...\")\n",
    "if 'Ridge' in best_overfitted_name:\n",
    "    unwatched_features_over_scaled = scaler_overfitted.transform(unwatched_features_over)\n",
    "    raw_predictions = best_overfitted_model.predict(unwatched_features_over_scaled)\n",
    "else:\n",
    "    raw_predictions = best_overfitted_model.predict(unwatched_features_over)\n",
    "\n",
    "# PROPERLY CLIP PREDICTIONS TO LETTERBOXD SCALE (0.5 to 5.0)\n",
    "clipped_predictions = np.clip(raw_predictions, 0.5, 5.0)\n",
    "\n",
    "print(f\"Raw prediction range: {raw_predictions.min():.2f} to {raw_predictions.max():.2f}\")\n",
    "print(f\"Clipped prediction range: {clipped_predictions.min():.2f} to {clipped_predictions.max():.2f}\")\n",
    "\n",
    "# Add clipped predictions to dataframe\n",
    "unwatched_overfitted['predicted_rating'] = clipped_predictions\n",
    "\n",
    "# Filter for high predicted ratings (3.5+ stars to see more variety)\n",
    "high_rated_overfitted = unwatched_overfitted[unwatched_overfitted['predicted_rating'] >= 3.5].copy()\n",
    "\n",
    "# Sort and get recommendations\n",
    "overfitted_recommendations = high_rated_overfitted.nlargest(100, 'predicted_rating')\n",
    "\n",
    "print(f\"🎯 Found {len(high_rated_overfitted)} movies predicted 3.5+ stars\")\n",
    "print(f\"🎯 Found {len(unwatched_overfitted[unwatched_overfitted['predicted_rating'] >= 4.0])} movies predicted 4.0+ stars\")\n",
    "print(f\"🎯 Found {len(unwatched_overfitted[unwatched_overfitted['predicted_rating'] >= 4.5])} movies predicted 4.5+ stars\")\n",
    "\n",
    "# Display top overfitted recommendations\n",
    "print(f\"\\n🧠 TOP 25 OVERFITTED ML RECOMMENDATIONS:\")\n",
    "overfitted_final_recs = []\n",
    "\n",
    "for i, (_, movie) in enumerate(overfitted_recommendations.iterrows(), 1):\n",
    "    title = movie['movie_title'] \n",
    "    year = int(movie['year_released']) if pd.notna(movie['year_released']) else 'Unknown'\n",
    "    pred_rating = movie['predicted_rating']\n",
    "    \n",
    "    # Get genre info if available\n",
    "    genre_info = \"\"\n",
    "    if 'genre_string' in movie and pd.notna(movie['genre_string']) and movie['genre_string']:\n",
    "        genres = movie['genre_string'][:50] + \"...\" if len(str(movie['genre_string'])) > 50 else movie['genre_string']\n",
    "        genre_info = f\" | {genres}\"\n",
    "    \n",
    "    overfitted_final_recs.append({\n",
    "        'rank': i,\n",
    "        'title': title,\n",
    "        'year': year,\n",
    "        'predicted_rating': pred_rating,\n",
    "        'tmdb_id': movie['tmdb_id'],\n",
    "        'genre_info': genre_info\n",
    "    })\n",
    "    \n",
    "    if i <= 25:  # Show top 25\n",
    "        print(f\"{i:2d}. {title} ({year}) - {pred_rating:.2f}/5.0{genre_info}\")\n",
    "\n",
    "# Compare prediction distributions\n",
    "print(f\"\\n📊 OVERFITTED PREDICTION ANALYSIS:\")\n",
    "print(f\"Your actual rating distribution:\")\n",
    "actual_dist = ml_data_clean['Rating'].value_counts().sort_index()\n",
    "for rating, count in actual_dist.items():\n",
    "    print(f\"   {rating:.1f} stars: {count:3d} movies ({count/len(ml_data_clean)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPredicted rating distribution (all unwatched movies):\")\n",
    "pred_bins = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "pred_hist = pd.cut(clipped_predictions, bins=pred_bins + [6.0]).value_counts().sort_index()\n",
    "for interval, count in pred_hist.items():\n",
    "    if count > 0:\n",
    "        print(f\"   {interval}: {count:5d} movies ({count/len(clipped_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Save overfitted recommendations with genre info\n",
    "overfitted_filename = 'data/movie_recommendations_OVERFITTED_with_genres.txt'\n",
    "with open(overfitted_filename, 'w') as f:\n",
    "    f.write(\"🧠 OVERFITTED ML RECOMMENDATIONS (with Genre Info)\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(f\"Model: {best_overfitted_name}\\n\")\n",
    "    f.write(f\"Features: {len(non_zero_variance_cols)} engineered features including genres\\n\")\n",
    "    f.write(f\"Training data: {len(ml_data_clean)} of your rated movies\\n\")\n",
    "    f.write(f\"Cross-validation RMSE: {overfitted_results[best_overfitted_name]['CV_RMSE']:.4f}\\n\")\n",
    "    f.write(f\"🎯 DESIGNED TO OVERFIT TO YOUR SPECIFIC TASTE\\n\")\n",
    "    f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    # Write different rating tiers\n",
    "    for min_rating in [4.5, 4.0, 3.5]:\n",
    "        tier_movies = [m for m in overfitted_final_recs if m['predicted_rating'] >= min_rating]\n",
    "        if tier_movies:\n",
    "            f.write(f\"🌟 TIER {min_rating}+ STARS ({len(tier_movies)} movies):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            for movie in tier_movies[:20]:  # Top 20 per tier\n",
    "                f.write(f\"{movie['rank']:3d}. {movie['title']} ({movie['year']})\\n\")\n",
    "                f.write(f\"     🤖 Predicted: {movie['predicted_rating']:.2f}/5.0{movie['genre_info']}\\n\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Saved {len(overfitted_final_recs)} OVERFITTED recommendations to '{overfitted_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
